<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>JBake</title>
    <link>http://jprante.github.io</link>
    <atom:link href="http://jprante.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <description>JBake Bootstrap Template</description>
    <language>en-gb</language>
    <pubDate>Di, 6 Jan 2015 22:22:04 +0100</pubDate>
    <lastBuildDate>Di, 6 Jan 2015 22:22:04 +0100</lastBuildDate>

    
    <item>
      <title>Bintray for Elasticsearch plugins</title>
      <link>http://jprante.github.io/2013/01/18/Bintray-for-Elasticsearch-plugins.html</link>
      <pubDate>So, 13 Jan 2013 00:00:00 +0100</pubDate>
      <guid isPermaLink="false">2013/01/18/Bintray-for-Elasticsearch-plugins.html</guid>
      <description>
      &lt;div class=&quot;imageblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;/images/puzzle_kindergeschirr_ar.jpg&quot; alt=&quot;puzzle kindergeschirr ar&quot;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;design wendy boudewijns 2006&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_publishing_plugins&quot;&gt;Publishing plugins&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The development of Elasticsearch plugins is a wonderful thing. Once the plugin is ready, you can be sure the large
community can download the zip archive and install it with the ./bin/plugin command line tool which comes with
every Elasticsearch distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Plugins are the preferred method for decentralized, voluntary development of Elasticsearch extensions.
There are a lot of plugins that extend the power of Elasticsearch. And, it is very easy to install them all.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is the easy way of doing things that stimulates me to develop and publish plugins, but Github suddenly
roused me from slumber &lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_1&quot; class=&quot;footnote&quot; href=&quot;#_footnote_1&quot; title=&quot;View footnote.&quot;&gt;1&lt;/a&gt;]&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_github_terminates_binary_service&quot;&gt;Github terminates binary service&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the past, it was the Github platform which offered a download area beside the source code management.
An open source developer could upload additional files to the github code. Because it was so convenient,
Elasticsearch was extended to automatic downloads of zip archives from github. But that is history now.
I understand Github wants to focus on their strength, source code management.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Instead, Elasticsearch&amp;#8217;s plugin command was modified to handle downloads from
&lt;a href=&quot;http://download.elasticsearch.org&quot; class=&quot;bare&quot;&gt;http://download.elasticsearch.org&lt;/a&gt; for the core plugins, and by using search frontends on &quot;Maven Central&quot;
for zip archives &lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_2&quot; class=&quot;footnote&quot; href=&quot;#_footnote_2&quot; title=&quot;View footnote.&quot;&gt;2&lt;/a&gt;]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Using elasticsearch.org as a download platform could be an alternative to move my plugin business to.
But I think this should be private to the core plugins developed by Elasticsearch.
And, third-party code hosting needs some careful precautions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;What about downloading from Maven? Creating zip archives on Maven Central is tedious because&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;you must sign all artifacts with a PGP signature&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;you must own the domain of the Maven groupId&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maven Central is not only Apache Software Foundation but includes also third-party
approved repository
hosting facilities I don&amp;#8217;t want to use for my software distribution
&lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_3&quot; class=&quot;footnote&quot; href=&quot;#_footnote_3&quot; title=&quot;View footnote.&quot;&gt;3&lt;/a&gt;]&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;my artifacts often depend on customized and improved builds and not on some broken
artifacts on Maven Central&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So, Maven would be a viable solution if Elasticsearch would embrace the full power of dependency resolution.
I made up a demonstrator &lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_4&quot; class=&quot;footnote&quot; href=&quot;#_footnote_4&quot; title=&quot;View footnote.&quot;&gt;4&lt;/a&gt;]&lt;/span&gt;
for an Elasticsearch version that can load &quot;apps&quot; from various repositories by configuration,
just when a node starts up. But that&amp;#8217;s still the future.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So, what to do? For developement just for fun, maintaining a server for deploying plugins is quite expensive.
The next would be to set up an HTTP server for download. More, I&amp;#8217;d have to watch the server regularly for availablity,
because I want a reliable service for the Elasticsarch  community. Short story, I&amp;#8217;m out of time to do it in my spare time.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_bintray&quot;&gt;Bintray&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I was happy when I learned about Bintray last December. Bintray is a social platform for the storage and
distribution of software binaries. The cloud-based platform is offered to open source developers free of charge.
It helps in the process of making binaries publicly available. The company behind Bintray is JFrog, known
from Artifactory, a competitor to Sonatype Nexus, and is well established in the business of providing Maven
repository infrastructure.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I was excited being invited to the non-public beta a few weeks before. Now after the public beta is open,
Bintray was also mentioned in blogs &lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_5&quot; class=&quot;footnote&quot; href=&quot;#_footnote_5&quot; title=&quot;View footnote.&quot;&gt;5&lt;/a&gt;]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Bintray allows importing Github accounts, so it was instantly ready to setup for my github projects.
It&amp;#8217;s amazing to see these services working hand in hand! Adding a project &apos;elasticsearch-plugins&apos; was a matter of seconds.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The modification to the Elasticsearch plugin build process is small. I received an API key and added
an entry in ~/.m2/settings.xml for the bintray API server. By adding the distribution information&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre&gt;     &amp;lt;distributionManagement&amp;gt;
        &amp;lt;repository&amp;gt;
            &amp;lt;id&amp;gt;bintray-jprante-elasticsearch-plugins-elasticsearch-river-jdbc&amp;lt;/id&amp;gt;
            &amp;lt;name&amp;gt;jprante-elasticsearch-plugins-elasticsearch-river-jdbc&amp;lt;/name&amp;gt;
            &amp;lt;url&amp;gt;https://api.bintray.com/maven/jprante/elasticsearch-plugins/elasticsearch-river-jdbc/release&amp;lt;/url&amp;gt;
        &amp;lt;/repository&amp;gt;
    &amp;lt;/distributionManagement&amp;gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I could continue with a single command&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre&gt;    mvn deploy&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;for uploading the Maven build artifacts to Bintray. After upload, you have to confirm the publication of the artifacts.
By opening the home screen of Bintray, you can browse to the zip plugin and learn about the URL of the file.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Next, I added the URL (having it shortened by an URL shortener service) to the plugin README, for example&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre&gt;    ./bin/plugin -url http://bit.ly/U75w1N -install river-jdbc&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;and that&amp;#8217;s it!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;/images/bintray1.png&quot; alt=&quot;bintray1&quot;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Direct downloads from your &lt;a href=&quot;https://bintray.com/pkg/show/general/jprante/elasticsearch-plugins/elasticsearch-river-jdbc&quot;&gt;homepage&lt;/a&gt;
at Bintray helps others to validate downloads with the help of a SHA1 checksum.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And, Bintray comes with a Maven repository, so everybody can reuse my Elasticsearch plugin jar libraries for developing
better plugins.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note, I did neither had to sign the artifacts with PGP keys nor did I qualified to be an owner for my Maven groupId.
JFrog trusts me I&amp;#8217;m doing the right thing and that is really compelling.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;All I have to do is conform to the terms of service &lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_6&quot; class=&quot;footnote&quot; href=&quot;#_footnote_6&quot; title=&quot;View footnote.&quot;&gt;6&lt;/a&gt;]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I&amp;#8217;m not a lawyer and I don&amp;#8217;t know if the terms are valid here in Germany, but I treat them with respect.
There are pros and cons in the terms of service, but my hope is that Bintray will keep up free service for open source
developers and will not discontinue or limit download space or availability in the near future.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The next thing I want to explore is the great devops-incubator of Henri Gomez, for producing Elasticsearch RPMs.
Bintray provides not only Maven repository but also RPM/yum. Great news because my favorite Linux is Red Hat Linux.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So, thumbs up, I wish I had finally found the place where my binaries could finally settle, and solve my binary
software distribution delivery challenge being an individual developer. As a bonus I get some nice statistics and
comment feedback from Bintray.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Having saved money I bought some AMD 64bit/8GB/SATA3/SSD/1g eth machines for Elasticsearch benchmarking at home
(more neat stuff coming).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;footnotes&quot;&gt;
&lt;hr&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_1&quot;&gt;
&lt;a href=&quot;#_footnoteref_1&quot;&gt;1&lt;/a&gt; &lt;a href=&quot;https://github.com/blog/1302-goodbye-uploads&quot; class=&quot;bare&quot;&gt;https://github.com/blog/1302-goodbye-uploads&lt;/a&gt;
&lt;/div&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_2&quot;&gt;
&lt;a href=&quot;#_footnoteref_2&quot;&gt;2&lt;/a&gt; &lt;a href=&quot;http://www.elasticsearch.org/blog/2012/12/17/new-download-service.html&quot; class=&quot;bare&quot;&gt;http://www.elasticsearch.org/blog/2012/12/17/new-download-service.html&lt;/a&gt; The hardcoded URLs for looking for zip files in Maven repos are &lt;a href=&quot;http://search.maven.org/remotecontent?filepath=&quot; class=&quot;bare&quot;&gt;http://search.maven.org/remotecontent?filepath=&lt;/a&gt; and &lt;a href=&quot;https://oss.sonatype.org/service/local/repositories/releases/content/&quot; class=&quot;bare&quot;&gt;https://oss.sonatype.org/service/local/repositories/releases/content/&lt;/a&gt;
&lt;/div&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_3&quot;&gt;
&lt;a href=&quot;#_footnoteref_3&quot;&gt;3&lt;/a&gt; &lt;a href=&quot;http://maven.apache.org/guides/mini/guide-central-repository-upload.html&quot; class=&quot;bare&quot;&gt;http://maven.apache.org/guides/mini/guide-central-repository-upload.html&lt;/a&gt;
&lt;/div&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_4&quot;&gt;
&lt;a href=&quot;#_footnoteref_4&quot;&gt;4&lt;/a&gt; &lt;a href=&quot;https://github.com/jprante/elasticsearch-apps&quot; class=&quot;bare&quot;&gt;https://github.com/jprante/elasticsearch-apps&lt;/a&gt;
&lt;/div&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_5&quot;&gt;
&lt;a href=&quot;#_footnoteref_5&quot;&gt;5&lt;/a&gt; &lt;a href=&quot;http://www.drdobbs.com/cloud/need-a-new-binary-ask-a-developer-first/240146444&quot; class=&quot;bare&quot;&gt;http://www.drdobbs.com/cloud/need-a-new-binary-ask-a-developer-first/240146444&lt;/a&gt;
&lt;/div&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_6&quot;&gt;
&lt;a href=&quot;#_footnoteref_6&quot;&gt;6&lt;/a&gt; &lt;a href=&quot;https://bintray.com/docs/terms_of_service.html&quot; class=&quot;bare&quot;&gt;https://bintray.com/docs/terms_of_service.html&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
	  </description>
    </item>
    
    <item>
      <title>Elasticsearch Java Virtual Machine settings explained</title>
      <link>http://jprante.github.io/2012/11/28/Elasticsearch-Java-Virtual-Machine-settings-explained.html</link>
      <pubDate>Mi, 28 Nov 2012 00:00:00 +0100</pubDate>
      <guid isPermaLink="false">2012/11/28/Elasticsearch-Java-Virtual-Machine-settings-explained.html</guid>
      <description>
      &lt;div class=&quot;imageblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;/images/toy-domo-domo-kun-480x800.jpg&quot; alt=&quot;toy domo domo kun 480x800&quot;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Today, it finally happened. Java 6 (&quot;Mustang&quot;), which appeared early 2006 and is still in production at many places,
has reached end of life.
There is no longer a reason not to transition to Java 7 (&quot;Dolphin&quot;)
&lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_1&quot; class=&quot;footnote&quot; href=&quot;#_footnote_1&quot; title=&quot;View footnote.&quot;&gt;1&lt;/a&gt;]&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This occasion inspired me to write a blog post about the intriguing details about the subtleties of
Java 6 and 7 Virtual Machine settings and Elasticsearch.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Elasticsearch comes with &lt;strong&gt;some preconfigured settings&lt;/strong&gt; for the Java Virtual Machine (JVM).
Normally, because they have been chosen very carefully, you don&amp;#8217;t need to care much about them, and you can use
Elasticseach right away.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But it may happen, while monitoring the memory of your Elasticsearch nodes, you could get tempted by changing
some of the settings. Will it improve your situation or not?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This blog tries to demystify the preconfigured Elasticsearch settings and discusses the most common adjustments. Finally, some recommendations are given how tunings should be undertaken.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;h2. Overview of the Elasticsearch JVM settings&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;These are the preconfigured settings for Elasticearch 0.19.11.&lt;/p&gt;
&lt;/div&gt;
&lt;table class=&quot;tableblock frame-all grid-all spread&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 33%;&quot;&gt;
&lt;col style=&quot;width: 33%;&quot;&gt;
&lt;col style=&quot;width: 33%;&quot;&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;JVM parameter&lt;/th&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;Elasticsearch default value&lt;/th&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;Environment variable&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;-Xms&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;256m&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;ES_MIN_MEM&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;-Xmx&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;1g&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;ES_MAX_MEM&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;-Xms and -Xmx&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;ES_HEAP_SIZE&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;-Xmn&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;ES_HEAP_NEWSIZE&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;-XX:MaxDirectMemorySize&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;ES_DIRECT_SIZE&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;-Xss&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;256k&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;-XX:UseParNewGC&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;+&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;-XX:UseConcMarkSweepGC&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;+&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;-XX:CMSInitiatingOccupancyFraction&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;75&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;-XX:UseCMSInitiatingOccupancyOnly&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;+&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;-XX:UseCondCardMark&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;(commented out)&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The first you notice is that Elasticsearch reserves the JVM heap memory between &lt;strong&gt;256 MB&lt;/strong&gt; and &lt;strong&gt;1 GB&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This setup is for development and demonstration environments. Developers can install Elasticsearch by simply unzipping
the distribution package and executing @./bin/elasticsearch -f@ from the command line. While this is great for
development and works in many cases
&lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_2&quot; class=&quot;footnote&quot; href=&quot;#_footnote_2&quot; title=&quot;View footnote.&quot;&gt;2&lt;/a&gt;]&lt;/span&gt;,
it does not suffice in situations where you need more memory for your Elasticsearch workload, and you have
far more than 2 GB RAM available.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;ES_MIN_MEM/ES_MAX_MEM&lt;/strong&gt; control the heap settings. The new variable is &lt;strong&gt;ES_HEAP_SIZE&lt;/strong&gt;, it is a more convenient choice because it sets the same value for the start and the max heap size. This suggests the JVM &lt;strong&gt;not to use fragments of memory&lt;/strong&gt; for heap allocation, if possible. Memory fragmentation is bad for maximum performance.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;ES_HEAP_NEWSIZE&lt;/strong&gt; is an optional parameter, it controls a subset of the heap, the size for the young generation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;ES_DIRECT_SIZE&lt;/strong&gt; controls the direct memory, the area where the JVM manages the data that is used in the NIO framework. Direct memory can be mapped to the virtual address space, which is more efficient on machines with 64bit architecture, since it circumvents the filesystem buffer. Elasticsearch opts for &lt;strong&gt;no restriction for direct memory&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are several collectors in Java JVMs for historical reasons. They can be enabled by the following combination of JVM parameters:&lt;/p&gt;
&lt;/div&gt;
&lt;table class=&quot;tableblock frame-all grid-all spread&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 50%;&quot;&gt;
&lt;col style=&quot;width: 50%;&quot;&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;JVM parameter&lt;/th&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;Garbage collector&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;-XX:+UseSerialGC&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;serial collector&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;-XX:+UseParallelGC&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;parallel collector&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;-XX:+UseParallelOldGC&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Parallel compacting collector&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;-XX:+UseConcMarkSweepGC&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Concurrent-Mark-Sweep (CMS) collector&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;-XX:+UseG1GC&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Garbage-First collector (G1)&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;UseParNewGC&lt;/strong&gt; and &lt;strong&gt;UseConcMarkSweepGC&lt;/strong&gt; combines both parallelism and concurrency in the garbage collector. &lt;strong&gt;UseConcMarkSweepGC&lt;/strong&gt; automatically selects &lt;strong&gt;UseParNewGC&lt;/strong&gt; and this disables the serial collector. This is the default since Java 6.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;CMSInitiatingOccupancyFraction&lt;/strong&gt; refines a Concurrent-Mark-Sweep garbage collector setting; it sets the old generation occupancy threshold that triggers collections to 75. The size of the old generation is the size of the heap minus the size of the new generation. This tells the JVM to always start a garbage collection when the heap is 75% full. This is an estimated value, since smaller heaps may need earlier GC starts.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;UseCondCardMark&lt;/strong&gt; would issue an additional check in the card table entry use in the garbage collector before storing a value. UseCondCardMark does not affect the Garbage-First collector.
It is recommended in highly concurrent environments
&lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_3&quot; class=&quot;footnote&quot; href=&quot;#_footnote_3&quot; title=&quot;View footnote.&quot;&gt;3&lt;/a&gt;]&lt;/span&gt;
. In Elasticsearch, it is listed as a commented out value.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Some of these settings were examined by efforts in projects like Apache Cassandra that have similar resource demands
like Elasticsearch regarding the JVM
For examples, see &lt;a href=&quot;http://www.datastax.com/dev/blog/whats-new-cassandra-066&quot;&gt;http://www.datastax.com/dev/blog/whats-new-cassandra-066&lt;/a&gt;
&lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_4&quot; class=&quot;footnote&quot; href=&quot;#_footnote_4&quot; title=&quot;View footnote.&quot;&gt;4&lt;/a&gt;]&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In summary, the Elasticsearch preconfigured settings&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;override the automatic heap memory settings of the JVM in favor of adjusting the &lt;strong&gt;maximum default heap size&lt;/strong&gt; to a mere &lt;strong&gt;1GB&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;assume that garbage collection runs at &lt;strong&gt;75 percent allocation of your heap size&lt;/strong&gt; will not interfere with your workload performance requirements&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;disable the &lt;strong&gt;Java 7 default G1 collector&lt;/strong&gt; if you run Elasticsearch on Java 7 later than &lt;strong&gt;7u4&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_the_memory_structure_of_a_jvm_process&quot;&gt;The memory structure of a JVM process&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Before we can give some rules of thumb for Elasticsearch tuning, we discuss some concepts of the JVM, so we can explain why the preconfigured settings in the Elasticsearch distribution make sense.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For now, we assume the Sun/Oracle HotSpot JVM in the discussion. Elasticseach can run under JVMs of different vendors, which is interesting, but this is not the topic of this post.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;/images/jvm.png&quot; alt=&quot;jvm&quot;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The JVM memory consists of several segments.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the &lt;strong&gt;JVM code&lt;/strong&gt; itself, with internal code and data, and internal interfaces, like profiling and monitoring agents or bytecode instrumentation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the &lt;strong&gt;non-heap memory&lt;/strong&gt;, where classes are loaded&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the &lt;strong&gt;stack memory&lt;/strong&gt;, where frames (local variables and operands for each thread) live&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the &lt;strong&gt;heap memory&lt;/strong&gt;, where handles (object references) and objects live&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the &lt;strong&gt;direct buffer&lt;/strong&gt;, where buffers for direct input/output of data is stored&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The heap memory is an important size, because Java depends on a reasonably sized heap, and the JVM can be instructed to grab only a limited amount of memory for the heap from the operating system that will be reserved during the JVM lifetime.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If the heap is &lt;strong&gt;too small&lt;/strong&gt;, many garbage collections are run and chances are higher to encounter OutOfMemory exceptions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If the heap is &lt;strong&gt;too large&lt;/strong&gt;, garbage collections are delayed, but if they run, the algorithm is challenged to cope with a higher number of live heap data. And, the operating system will be put under stress, the chance of paging the JVM process is higher with large heaps.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note, with the Concurent-Mark-Sweep garbage collector, Java does not release memory back to the operating system, so it is very important to configure a reasonable heap start size and a maximum heap size.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The non-heap memory is allocated by the Java application automatically. There is no way to control this amount by a parameter because it is determined by the size (the footprint) of the Java application code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The stack memory is allocated per thread. In Elasticsearch, the size of the stack per thread had to be increased
from &lt;strong&gt;128k&lt;/strong&gt; to &lt;strong&gt;256k&lt;/strong&gt;, because Java 7 stack frames are larger than in Java 6. One reason is that Java 7 supports
new programming language features that utilize space on stack frames. For example, continuations,
a concept known from functional programming languages, have been introduced
&lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_5&quot; class=&quot;footnote&quot; href=&quot;#_footnote_5&quot; title=&quot;View footnote.&quot;&gt;5&lt;/a&gt;]&lt;/span&gt;
Continuations are useful for coroutines, for green threads, or for fibers
&lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_6&quot; class=&quot;footnote&quot; href=&quot;#_footnote_6&quot; title=&quot;View footnote.&quot;&gt;6&lt;/a&gt;]&lt;/span&gt;.
When implementing non-blocking I/O, the big advantage is, code can be written as if threads were used, but the runtime
will use non-blocking I/O under the hood. Elasticsearch makes use of several thread pools. Because the Netty I/O
framework and Guava are base components of Elasticsearch, there is a potential to exploit the new threading
optimization features of Java 7 under the hood.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is a challenge to find a good value for the maximum stack size, because the stack space consumption can&amp;#8217;t be
easily compared between different CPU architectures and operating systems, not even between JVM versions.
There is a vendor builtin stack size default in the stock JVM that depends on the CPU architecture and the operating
system. But are they really suitable under all conditions? E.g. Solaris Sparc 64bit has a JVM Xss default of 512k
because of the larger address pointers, and Solaris x86 has 320k. Linux has lower limits down to 256k. Windows 32bit
with Java 6 has a default of 320k and Windows 64bit even 1024k stack space.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_the_large_heap_challenge&quot;&gt;The Large Heap Challenge&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Today, several GB of RAM are common. But not long time ago, your sysadmins broke out in tears if you asked them for
some extra gigabyte of main memory for your latest J2EE app.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Java garbage collectors were improved by the advent of Java 6 in 2006. Since then, they were able to run many
tasks in parallel and reduced the pauses, the stop-the-world phases.
The Concurrent-mark-sweep algorithm is a generational, mostly concurrent, parallel,
non-moving garbage collection &lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_7&quot; class=&quot;footnote&quot; href=&quot;#_footnote_7&quot; title=&quot;View footnote.&quot;&gt;7&lt;/a&gt;]&lt;/span&gt;.
But unfortunately, it does not scale with the number of live data on the heap.
Prateek Khanna and Aaron Morton gave numbers about how large a heap can be handled by the CMS garbage collector
&lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_8&quot; class=&quot;footnote&quot; href=&quot;#_footnote_8&quot; title=&quot;View footnote.&quot;&gt;8&lt;/a&gt;]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_avoiding_stop_the_world_phases&quot;&gt;Avoiding Stop-the-world phases&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We learned that Elasticsearch preconfigured the CMS garbage collector. This does not prevent long GC pauses, it only reduces the probability. CMS is a low pause collector, but still has some edge cases. When large megabyte arrays are on the heap, or in emergency cases, CMS may take much more time than expected
&lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_9&quot; class=&quot;footnote&quot; href=&quot;#_footnote_9&quot; title=&quot;View footnote.&quot;&gt;9&lt;/a&gt;]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The creation of megabyte-sized arrays are common to Lucene segment-based indexing when it comes to segment merging. If you want to try to reduce some extra load from the CMS garbage collector, adjust the number of segments for the Lucene merging in the parameter @index.merge.policy.segments_per_tier@
&lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_10&quot; class=&quot;footnote&quot; href=&quot;#_footnote_10&quot; title=&quot;View footnote.&quot;&gt;10&lt;/a&gt;]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_minimize_paging&quot;&gt;Minimize paging&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The risk of a large heap is higher memory pressure. Note, if the Java JVM operates with a large heap, this memory is no longer available to the rest of the system. If memory gets tight, operating systems tend to react with paging (swapping), and, in emergency situations, when all other methods of reclaiming memory failed, even with process killing. If paging occurs, overall performance will degrade, and also garbage collection performance degrades. So, don&amp;#8217;t allocate too much memory for the heap.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_the_garbage_first_alternative&quot;&gt;The Garbage first alternative&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Since Java JDK 7u4, the Garbage-First (G1) collector is the default garbage collector of Java 7. It is targeted for multi-processor machines with very huge memory. It meets low pause time goals with high probability. Whole-heap operations, such as global marking, are performed concurrently with the application threads. This prevents interruptions proportional to heap or live-data size.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The G1 collector is targeted to gain more throughput, not more speed &lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_11&quot; class=&quot;footnote&quot; href=&quot;#_footnote_11&quot; title=&quot;View footnote.&quot;&gt;11&lt;/a&gt;]&lt;/span&gt;. It works best if&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;more than &lt;strong&gt;50%&lt;/strong&gt; of the Java heap is occupied with live data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the rate of object allocation rate or promotion varies significantly&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;undesired &lt;strong&gt;long garbage collection or compaction pauses&lt;/strong&gt; (longer than 0.5 to 1 second) take place&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note, if using G1 garbage collector, it can be expected that memory no longer used for the heap &lt;strong&gt;might be given back to the operating system&lt;/strong&gt;.
The disadvantage of the G1 collector is less application performance due to higher CPU utilization.
So, with enough RAM and average CPU power, CMS can hold the edge over G1
&lt;span class=&quot;footnote&quot;&gt;[&lt;a id=&quot;_footnoteref_12&quot; class=&quot;footnote&quot; href=&quot;#_footnote_12&quot; title=&quot;View footnote.&quot;&gt;12&lt;/a&gt;]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For Elasticsearch, the G1 means no long stop-the-world pauses and more flexible memory management, because buffer memory and system caches for input/output can better utilize the RAM resources of the machine. This comes at the price of &lt;strong&gt;less maximum performance&lt;/strong&gt; because G1 utilizes more CPU.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_strategies_against_performance_degradation&quot;&gt;Strategies against performance degradation&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So, maybe you read this blog post because you want to get hints how to tackle performance degradation, and you are sure your current heap settings might be one of the cause.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;First, make yourself clear about your performance strategy. Do you want maximum speed or maximum throughput?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;log everything and collect statistics and settings for complete diagnostics&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;read log files and analyze events&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;select your tuning target (maximum performance or maximum throughput)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;plan your tweaking&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;apply your new settings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;enable monitoring of the system with the new settings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;repeat from the beginning if new settings did not improve the situation&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_elasticsearch_garbage_collection_logging_format&quot;&gt;Elasticsearch Garbage collection logging format&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Elasticsearch warns in the logs about long garbage collection runs like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre&gt;[2012-11-26 18:13:53,166][WARN ][monitor.jvm              ] [Ectokid] [gc][ParNew][1135087][11248] duration [2.6m], collections [1]/[2.7m], total [2.6m]/[6.8m], memory [2.4gb]-&amp;gt;[2.3gb]/[3.8gb], all_pools {[Code Cache] [13.7mb]-&amp;gt;[13.7mb]/[48mb]}{[Par Eden Space] [109.6mb]-&amp;gt;[15.4mb]/[1gb]}{[Par Survivor Space] [136.5mb]-&amp;gt;[0b]/[136.5mb]}{[CMS Old Gen] [2.1gb]-&amp;gt;[2.3gb]/[2.6gb]}{[CMS Perm Gen] [35.1mb]-&amp;gt;[34.9mb]/[82mb]}&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The usage is documented in the class JvmMonitorService.&lt;/p&gt;
&lt;/div&gt;
&lt;table class=&quot;tableblock frame-all grid-all spread&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 50%;&quot;&gt;
&lt;col style=&quot;width: 50%;&quot;&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;Logfile&lt;/th&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;Explanation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;code&gt;gc&lt;/code&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;garbage collection run&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;code&gt;ParNew&lt;/code&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;new parallel garbage collector&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;code&gt;duration 2.6m&lt;/code&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Garbage collection took 2.6 minutes&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;code&gt;collections [1]/[2.7m]&lt;/code&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;one collection run, with a total of 2.7 minutes&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;code&gt;memory [2.4gb]&amp;#8594;[2.3gb]/[3.8gb]&lt;/code&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;usage number of pool &apos;memory&apos;, it was previously 2.4gb, now is 2.3gb, with a total pool size of 3.8gb&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;code&gt;Code Cache [13.7mb]&amp;#8594;[13.7mb]/[48mb]&lt;/code&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;usage numbers for pool &apos;code cache&apos;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;code&gt;Par Eden Space [109.6mb]&amp;#8594;[15.4mb]/[1gb]&lt;/code&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;usage numbers for pool &apos;Par Eden Space&apos;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;code&gt;Par Survivor Space [136.5mb]&amp;#8594;[0b]/[136.5mb]&lt;/code&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;usage numbers for pool &apos;Par Survivor Space&apos;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;code&gt;CMS Old Gen [2.1gb]&amp;#8594;[2.3gb]/[2.6gb]&lt;/code&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;usage numbers for pool &apos;CMS Old Gen&apos;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;code&gt;CMS Perm Gen [35.1mb]&amp;#8594;[34.9mb]/[82mb]&lt;/code&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;usage numbers for pool &apos;CMS Perm Gen&apos;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_some_recommendations&quot;&gt;Some recommendations&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Do not run Elasticsearch on a Java distribution before 6u22&lt;/strong&gt;. It will save you from subtle memory-related bugs. Bugs and deficiencies older than two or three years will hinder you from running Elasticsearch nodes flawlessly. Prefer Sun/Oracle distributions to old OpenJDK 6 distributions as they contain more bug fixes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Drop Java 6 and transition to Java 7.&lt;/strong&gt; Oracle announced the end-of-life of Java 6 and will only offer public updates until February, 2013.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Consider that Elasticsearch is a relatively new piece of software, using most advanced techniques for gaining performance. It squeezes &lt;strong&gt;everything out of the Java Virtual Machine&lt;/strong&gt;. Check the &lt;strong&gt;age of your operating system&lt;/strong&gt;. Running under the latest version of your operating system will always help your Java runtime environment to achieve the best performance.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Prepare yourself for &lt;strong&gt;regular Java runtime environment updates&lt;/strong&gt;, about each quarter of a year. Tell your sysadmins you are in need of updating the Java in your Elasticsearch installation from time to time to keep the pace with the ongoing improvements in Java execution performance. It is easy by doing &lt;strong&gt;rolling upgrades&lt;/strong&gt; without any downtimes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Begin small, end big.&lt;/strong&gt; Developing on a single Elasticsearch node is good. But remember, Elasticsearch is strong in multi-node deployments. A single node is not enough to make educated guesses about a production system running on dozens of nodes. At least, prepare hardware for three nodes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Test your workloads first before tuning the Java Virtual Machine.&lt;/strong&gt; Keep a test system for benchmarking your expected workloads. Create ingest and search workloads. Vary the number of nodes involved in your tests.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If your indexing workload is challenging, you may try to &lt;strong&gt;reduce the heap usage&lt;/strong&gt; in Elasticsearch indexing by adjusting the segment merging with the @index.merge.policy.segments_per_tier@ parameter.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Know your &lt;strong&gt;performance strategy&lt;/strong&gt; before tuning. Decide if you want to tune for &lt;strong&gt;maximum speed&lt;/strong&gt; or for &lt;strong&gt;maximum throughput&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Enable logging&lt;/strong&gt; of the Java garbage collecting for better diagnostics. Evaluate the numbers carefully before tweaking your system.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To &lt;strong&gt;enhance the CMS garbage collector&lt;/strong&gt;, you might add a reasonable -XX:CMSWaitDuration parameter.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you operate with &lt;strong&gt;very large heaps of more than 6-8 GB&lt;/strong&gt;, that is higher than the size the CMS garbage collector was designed for, and you encounter &lt;strong&gt;unacceptable long stop-the-world pauses&lt;/strong&gt;, you have several options. Adjust the &lt;strong&gt;CMSInitiatingOccupancyFraction&lt;/strong&gt; setting to reduce the probability of long GC runs, or try to &lt;strong&gt;reduce the maximum heap&lt;/strong&gt; under the limit, or enable the &lt;strong&gt;G1 garbage collector&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Learn about &lt;strong&gt;the art of Garbage Collection tuning&lt;/strong&gt;. If you want to become a master, output the list of available JVM options, together with the preconfigured values with the command @java -XX:+UnlockDiagnosticVMOptions -XX:+PrintFlagsFinal -version@ and start tweaking.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_references&quot;&gt;References&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The header image shows a &lt;a href=&quot;http://foundwalls.com/toy-domo-domo-kun&quot;&gt;Dōmo-kun&lt;/a&gt; (どーもくん)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The JVM memory diagram was taken from &lt;a href=&quot;http://sureshsvn.com/jvm.html&quot; class=&quot;bare&quot;&gt;http://sureshsvn.com/jvm.html&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_further_readings&quot;&gt;Further readings&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;http://openjdk.java.net/groups/hotspot/docs/StorageManagement.html&quot;&gt;OpenJDK Storage Management&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;http://labs.oracle.com/jtech/pubs/04-g1-paper-ismm.pdf&quot;&gt;David Detlefs, Christine Flood, Steve Heller, Tony Printezis: Garbage-First Garbage Collection&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;http://www.aioug.org/sangam12/Presentations/20155.pdf&quot;&gt;Prateek Khanna: Performance gain with G1 garbage collection algorithm in vertically scaled J2EE Infrastructure deployment&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;http://www.angelikalanger.com/Conferences/Slides/jf12_TuningTheHotSpotJVMsGarbageCollectors.pdf&quot;&gt;Angelika Langer, Klaus Kreft: The Art of Garbage Collection tuning&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;http://java.dzone.com/articles/how-tame-java-gc-pauses&quot;&gt;Alexey Ragozin, How to tame java GC pauses? Surviving 16GiB heap and greater&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;http://www.infoq.com/news/2010/04/cliff_click_gc_pauses&quot;&gt;Charles Humble, Keeping Garbage Collection Pauses Short&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;footnotes&quot;&gt;
&lt;hr&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_1&quot;&gt;
&lt;a href=&quot;#_footnoteref_1&quot;&gt;1&lt;/a&gt; &lt;a href=&quot;https://blogs.oracle.com/henrik/entry/updated_java_6_eol_date&quot;&gt;Updated Java 6 EOL date&lt;/a&gt;
&lt;/div&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_2&quot;&gt;
&lt;a href=&quot;#_footnoteref_2&quot;&gt;2&lt;/a&gt; It was &lt;a href=&quot;https://groups.google.com/forum/#!topic/elasticsearch/qmCI4sA6lPE&quot;&gt;reported&lt;/a&gt; that Elasticsearch can index 50 million documents with the default heap setting before the performance degraded near to a halt.
&lt;/div&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_3&quot;&gt;
&lt;a href=&quot;#_footnoteref_3&quot;&gt;3&lt;/a&gt; &lt;a href=&quot;http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7029167&quot;&gt;Bug 7029167&lt;/a&gt;
&lt;/div&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_4&quot;&gt;
&lt;a href=&quot;#_footnoteref_4&quot;&gt;4&lt;/a&gt; By default the JVM tries to estimate when it needs to begin a major compaction to strike a balance between on the one hand wasting CPU by performing GC before it was necessary and on the other running out of heap space before it can finish the collection, forcing it to fall back to a stop-the-world collection. (For gory details, the term you want to look up is concurrent mode failure.) For many applications this is fine, but with Cassandra it’s worth spending extra CPU to avoid even a small possibility of being paused for several seconds for a stop-the-world collection. These options tell the JVM to always start a collection when the heap is 75% full. (This is a reasonable default based on Cassandra deployments, but some workloads may need to begin GC even earlier, especially with relatively small heaps.)
&lt;/div&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_5&quot;&gt;
&lt;a href=&quot;#_footnoteref_5&quot;&gt;5&lt;/a&gt; &lt;a href=&quot;http://jcp.org/aboutJava/communityprocess/maintenance/jsr924/JVMSpec-JavaSE7-ChangeLog.html&quot;&gt;JSR 924&lt;/a&gt; specifies the Java Virtual machine for Java 7. Note Lukas Stadler&amp;#8217;s &quot;presentation&quot;:http://wiki.jvmlangsummit.com/images/2/2b/JVMLanguageSummit_Stadler_Continuations.pdf about JVM continuations.
&lt;/div&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_6&quot;&gt;
&lt;a href=&quot;#_footnoteref_6&quot;&gt;6&lt;/a&gt; &lt;a href=&quot;http://malhar.net/sriram/kilim/thread_of_ones_own.pdf&quot;&gt;Sriram Srinivasan, A Thread of One&amp;#8217;s Own&lt;/a&gt;
&lt;/div&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_7&quot;&gt;
&lt;a href=&quot;#_footnoteref_7&quot;&gt;7&lt;/a&gt; John Coomes, Tony Printezis, &quot;Performance Through Parallelism: Java HotSpot™ GC Improvements&quot;, JavaOne 2006
&lt;/div&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_8&quot;&gt;
&lt;a href=&quot;#_footnoteref_8&quot;&gt;8&lt;/a&gt; &lt;a href=&quot;http://permalink.gmane.org/gmane.comp.db.cassandra.user/23417&quot;&gt;&apos;The maximum useful heap size is 8GB.&apos;&lt;/a&gt; Aaron Morton, Apache Cassandra, DataStax MVP, &lt;a href=&quot;http://www.aioug.org/sangam12/Presentations/20155.pdf&quot;&gt;&apos;Concurrent Mark and Sweep Problems - larger heap sizes Xmx greater 6GB&apos;&lt;/a&gt; Prateek Khanna, Oracle
&lt;/div&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_9&quot;&gt;
&lt;a href=&quot;#_footnoteref_9&quot;&gt;9&lt;/a&gt; &quot;But in certain cases object may be allocated directly in old space and CMS cycle could start while Eden has lots of objects. In this case initial mark can be 10-100 times slower which is bad. Usually this is happening due to allocation of very large objects (few megabyte arrays). To avoid these long pauses you should configure reasonable –XX:CMSWaitDuration.&quot; &lt;a href=&quot;http://blog.griddynamics.com/2011/06/understanding-gc-pauses-in-jvm-hotspots_02.html&quot;&gt;Alexey Ragozin, Understanding GC pauses in JVM, HotSpot&amp;#8217;s CMS collector&lt;/a&gt;
&lt;/div&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_10&quot;&gt;
&lt;a href=&quot;#_footnoteref_10&quot;&gt;10&lt;/a&gt; Elasticsearch Guide, &lt;a href=&quot;http://www.elasticsearch.org/guide/reference/index-modules/merge.html&quot;&gt;&quot;Merge&quot;&lt;/a&gt;
&lt;/div&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_11&quot;&gt;
&lt;a href=&quot;#_footnoteref_11&quot;&gt;11&lt;/a&gt; Oracle, &lt;a href=&quot;http://docs.oracle.com/javase/7/docs/technotes/guides/vm/G1.html&quot;&gt;Garbage-First Collector&lt;/a&gt;
&lt;/div&gt;
&lt;div class=&quot;footnote&quot; id=&quot;_footnote_12&quot;&gt;
&lt;a href=&quot;#_footnoteref_12&quot;&gt;12&lt;/a&gt; A garbage collector exerciser source code is given by &lt;a href=&quot;http://nerds-central.blogspot.de/2011/11/comparing-java-7-garbage-collectors.html&quot;&gt;&quot;Dr Alexander J. Turner, Comparing Java 7 Garbage Collectors Under Extreme Load&quot;&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
	  </description>
    </item>
    
    <item>
      <title>Autocompletion with jQuery, JAX-RS and Elasticsearch</title>
      <link>http://jprante.github.io/applications/2012/08/17/Autocompletion-with-jQuery-JAX-RS-and-Elasticsearch.html</link>
      <pubDate>Fr, 17 Aug 2012 00:00:00 +0200</pubDate>
      <guid isPermaLink="false">applications/2012/08/17/Autocompletion-with-jQuery-JAX-RS-and-Elasticsearch.html</guid>
      <description>
      &lt;p&gt;&lt;img src=&quot;/images/autocomplete.jpeg&quot; alt=&quot;Typing&quot;&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;a href=&quot;http://www.flickr.com/photos/atomicshark/144630706/&quot;&gt;http://www.flickr.com/photos/atomicshark/144630706/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&lt;h2&gt;What is Autocompletion?&lt;/h2&gt;&lt;p&gt;Autocompletion is the provision of words that are frequently used in response to a user&apos;s keystrokes. It was first invented to assist people with physical disabilities to improve their typing speed. [1]&lt;/p&gt;&lt;p&gt;Autocomplete or word completion works so that when the writer writes the first letter or letters of a word, a program predicts one or more possible words as choices.&lt;/p&gt;&lt;p&gt;In this text, we will show how to implement a solution for autocompletion, a search frontend for Elasticsearch. It consists of a jQuery page, presenting a search form to the user, submitting query terms to a gateway service, and receiving JSON results for the autocompleted word list.&lt;/p&gt;&lt;p&gt;For our use case, we build a library catalog title autocompletion, where 80% of user queries are searches for title keywords. While typing, users should immediately got aware of titles matching their requests, and Elasticsearch should do the hard work to filter the relevant documents in near realtime.&lt;/p&gt;&lt;h2&gt;The n-gram method&lt;/h2&gt;&lt;p&gt;How will the autocompletion work? One approach is changing the normal indexing of title words. A well-known method is using n-grams.&lt;/p&gt;&lt;p&gt;In the fields of computational linguistics, an n-gram is a contiguous sequence of n items from a text sequence. N-gram matching implementation is simple and provides good performance. The algorithm is based on the principle: if a word A matches a word B containing some errors, they will most likely have at least one common substring of length &apos;n&apos;.&lt;/p&gt;&lt;p&gt;At indexing step, the word is partitioned into n-grams, the word is added to lists that correspond each of these n-grams. At search time, the query is also partitioned into n-grams, and for each of them corresponding lists are scanned using the metric.&lt;/p&gt;&lt;p&gt;One observation is that users of a library catalog enter characters from the front in the order the characters are written in the title word, so an effective improvement of n-grams is introducing &lt;em&gt;edged&lt;/em&gt; n-grams, where the word n-grams are always starting from one of the word edges, here, the front.&lt;/p&gt;&lt;h2&gt;The Elasticsearch setting&lt;/h2&gt;&lt;p&gt;Note, we use a multilingual library catalog setup (multilingual setup is not the focus now so details are not discussed now). We add edge n-gram analysis by a filter we call &lt;strong&gt;edgengramfilter&lt;/strong&gt;. It builds n-grams from the word front, with a minimum length of 2 characters and a maximum of 16 characters.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;settings&amp;quot; : {
  &amp;quot;index&amp;quot; : {
     &amp;quot;analysis&amp;quot; : {
        &amp;quot;filter&amp;quot; : {
           &amp;quot;germansnow&amp;quot; : {
              &amp;quot;type&amp;quot; : &amp;quot;snowball&amp;quot;,
              &amp;quot;language&amp;quot; : &amp;quot;German2&amp;quot;
           },
           &amp;quot;edgengramfilter&amp;quot; : {
               &amp;quot;type&amp;quot; : &amp;quot;edgeNgram&amp;quot;,
               &amp;quot;side&amp;quot; : &amp;quot;front&amp;quot;,
               &amp;quot;min_gram&amp;quot; : 2,
               &amp;quot;max_gram&amp;quot; : 16
           } 
        },
        &amp;quot;analyzer&amp;quot; : {
           &amp;quot;german&amp;quot; : {
              &amp;quot;type&amp;quot; : &amp;quot;custom&amp;quot;,
              &amp;quot;tokenizer&amp;quot; : &amp;quot;lowercase&amp;quot;,
              &amp;quot;filter&amp;quot; : &amp;quot;germansnow&amp;quot;                  
           },
           &amp;quot;icu&amp;quot; : {
              &amp;quot;type&amp;quot; : &amp;quot;custom&amp;quot;,
              &amp;quot;tokenizer&amp;quot; : &amp;quot;icu_tokenizer&amp;quot;,
              &amp;quot;filter&amp;quot; :  &amp;quot;icu_folding&amp;quot;

           },
           &amp;quot;default&amp;quot; : {
              &amp;quot;sub_analyzers&amp;quot; : [
                 &amp;quot;german&amp;quot;,
                 &amp;quot;icu&amp;quot;,
                 &amp;quot;standard&amp;quot;
              ],                  
              &amp;quot;type&amp;quot; : &amp;quot;combo&amp;quot;
           },
           &amp;quot;icu_autocomplete&amp;quot; : {
              &amp;quot;type&amp;quot; : &amp;quot;custom&amp;quot;,
              &amp;quot;tokenizer&amp;quot; : &amp;quot;icu_tokenizer&amp;quot;,
              &amp;quot;filter&amp;quot; : [ &amp;quot;icu_folding&amp;quot;, &amp;quot;edgengramfilter&amp;quot; ]
           },
           &amp;quot;autocomplete&amp;quot; : {
              &amp;quot;sub_analyzers&amp;quot; : [
                 &amp;quot;german&amp;quot;,
                 &amp;quot;icu_autocomplete&amp;quot;,
                 &amp;quot;standard&amp;quot;
              ],                  
              &amp;quot;type&amp;quot; : &amp;quot;combo&amp;quot;
           }
        }
     }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;Checking the edge n-gram creation&lt;/h2&gt;&lt;p&gt;With the analyzer API, we can observe the edge n-gram filter if it works like we want to. Note the maximum length of the grams should be relatively small. The number of extra words created is significant for the expected index size growth.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -XGET &amp;#39;localhost:9200/hbztest//_analyze?pretty&amp;amp;analyzer=icu_autocomplete&amp;amp;text=wissen&amp;#39;
{
  &amp;quot;tokens&amp;quot; : [ {
    &amp;quot;token&amp;quot; : &amp;quot;wi&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 2,
    &amp;quot;type&amp;quot; : &amp;quot;word&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;wis&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 3,
    &amp;quot;type&amp;quot; : &amp;quot;word&amp;quot;,
    &amp;quot;position&amp;quot; : 2
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;wiss&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 4,
    &amp;quot;type&amp;quot; : &amp;quot;word&amp;quot;,
    &amp;quot;position&amp;quot; : 3
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;wisse&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 5,
    &amp;quot;type&amp;quot; : &amp;quot;word&amp;quot;,
    &amp;quot;position&amp;quot; : 4
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;wissen&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 6,
    &amp;quot;type&amp;quot; : &amp;quot;word&amp;quot;,
    &amp;quot;position&amp;quot; : 5
  } ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now let&apos;s imagine we need to index a library catalog, where the title information is additionally directed to an autocompletion field &lt;strong&gt;xbib:titleAutocomplete&lt;/strong&gt;, for example&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;xbib:title&amp;quot; : {
   &amp;quot;type&amp;quot; : &amp;quot;string&amp;quot;,
   &amp;quot;type&amp;quot; : &amp;quot;multi_field&amp;quot;,
   &amp;quot;fields&amp;quot; : {
         &amp;quot;xbib:title&amp;quot;: {
             &amp;quot;type&amp;quot; : &amp;quot;string&amp;quot;,
             &amp;quot;index_name&amp;quot; : &amp;quot;dc:title&amp;quot;
         },
         &amp;quot;xbib:titleAutocomplete&amp;quot;: {
             &amp;quot;type&amp;quot; : &amp;quot;string&amp;quot;,
             &amp;quot;index_analyzer&amp;quot; : &amp;quot;icu_autocomplete&amp;quot;,
             &amp;quot;include_in_all&amp;quot; : false
         }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You don&apos;t need to touch your indexing code, just adding the field into the mapping is all Elasticsearch requires.&lt;/p&gt;&lt;h2&gt;An Autocompleter JAX-RS gateway&lt;/h2&gt;&lt;p&gt;A JAX-RS gateway is used to separate the jQuery and web design folks from direct access to the Elasticsearch cluster. Hence, you set up a middleware that is proxying valid autocomplete requests to the Elasticsearch backend that may be securely locked away in the darkest corners of your hosting facility.&lt;/p&gt;&lt;p&gt;In this working JAX-RS code, you can see how JSON data from an Elasticsearch query result can be generated directly into an JAX-RS streaming outputstream.&lt;br/&gt;The hard part of the work is encapsulated in service classes ElasticsearchSession and QueryResultAction which are not the focus of this article.&lt;/p&gt;&lt;p&gt;The autocompletion requests are formed by three input parameters: &lt;strong&gt;field&lt;/strong&gt; is the field to be queried in the Elasticsearch index, &lt;strong&gt;term&lt;/strong&gt; is the typed input of the user, and &lt;strong&gt;resultfields&lt;/strong&gt; are the fields that Elasticsearch should return. &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;package jaxrs;

import java.io.IOException;
import java.io.OutputStream;
import java.util.List;
import javax.servlet.http.HttpServletResponse;
import javax.ws.rs.FormParam;
import javax.ws.rs.POST;
import javax.ws.rs.Path;
import javax.ws.rs.Produces;
import javax.ws.rs.WebApplicationException;
import javax.ws.rs.core.Context;
import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.MultivaluedMap;
import javax.ws.rs.core.Response;
import javax.ws.rs.core.StreamingOutput;
import javax.ws.rs.core.UriInfo;
import javax.ws.rs.ext.ContextResolver;
import org.xbib.elasticsearch.ElasticsearchSession;
import org.xbib.elasticsearch.QueryResultAction;
import org.xbib.logging.Logger;
import org.xbib.logging.LoggerFactory;

/**
 * Elasticsearch autocomplete query bridge
 */
@Path(&amp;quot;/autocomplete&amp;quot;)
public class Autocompleter {

    @Context
    ContextResolver&amp;lt;ElasticsearchSession&amp;gt; resolver;
    private static final Logger logger = LoggerFactory.getLogger(&amp;quot;es.query.autocomplete&amp;quot;);
    private final QueryResultAction action = new QueryResultAction();

    @POST
    @Path(&amp;quot;/{index}&amp;quot;)
    @Produces(MediaType.APPLICATION_JSON)
    public StreamingOutput createIndexPage(
            @Context HttpServletResponse response,
            @Context UriInfo uriInfo,
            @FormParam(&amp;quot;field&amp;quot;) final String field,
            @FormParam(&amp;quot;term&amp;quot;) final String term,
            @FormParam(&amp;quot;resultfields[]&amp;quot;) final List&amp;lt;String&amp;gt; resultfields,            
            @FormParam(&amp;quot;filter&amp;quot;) final String filter,
            @FormParam(&amp;quot;from&amp;quot;) final int from,
            @FormParam(&amp;quot;size&amp;quot;) final int size)
            throws Exception {
        return query(response, uriInfo, field, term, resultfields, filter, from, size);
    }

    @POST
    @Path(&amp;quot;/{index}/{type}&amp;quot;)
    @Produces(MediaType.APPLICATION_JSON)
    public StreamingOutput createIndexTypePage(
            @Context HttpServletResponse response,
            @Context UriInfo uriInfo,
            @FormParam(&amp;quot;field&amp;quot;) final String field,
            @FormParam(&amp;quot;term&amp;quot;) final String term,
            @FormParam(&amp;quot;resultfields[]&amp;quot;) final List&amp;lt;String&amp;gt; resultfields,            
            @FormParam(&amp;quot;filter&amp;quot;) final String filter,
            @FormParam(&amp;quot;from&amp;quot;) final int from,
            @FormParam(&amp;quot;size&amp;quot;) final int size)
            throws Exception {
        return query(response, uriInfo, field, term, resultfields, filter, from, size);
    }

    private StreamingOutput query(
            final HttpServletResponse response,
            final UriInfo uriInfo,
            final String field,
            final String term,
            final List&amp;lt;String&amp;gt; resultfields,
            final String filter,
            final int from,
            final int size) throws IOException {
        MultivaluedMap&amp;lt;String, String&amp;gt; pathParams = uriInfo.getPathParameters();
        final String index = pathParams.getFirst(&amp;quot;index&amp;quot;);
        final String type = pathParams.getFirst(&amp;quot;type&amp;quot;);
        action.setQueryLogger(logger);
        action.setSession(resolver.getContext(ElasticsearchSession.class));
        action.setIndex(index);
        if (type != null) {
            action.setType(type);
        }
        action.setFrom(from);
        action.setSize(size);
        return new StreamingOutput() {
            @Override
            public void write(OutputStream output) throws IOException, WebApplicationException {
                if (field == null) {
                    throw new WebApplicationException(Response.status(400).entity(&amp;quot;field must not be null&amp;quot;).build());
                }
                if (term == null) {
                    throw new WebApplicationException(Response.status(400).entity(&amp;quot;term must not be null&amp;quot;).build());
                }
                if (resultfields == null) {
                    throw new WebApplicationException(Response.status(400).entity(&amp;quot;resultfields must not be null&amp;quot;).build());
                }
                action.setTarget(output);
                StringBuilder sb = new StringBuilder();
                if (filter != null) {
                    sb.append(&amp;quot;{\&amp;quot;fields\&amp;quot;:&amp;quot;);
                    append(sb, resultfields);
                    sb.append(&amp;quot;,\&amp;quot;query\&amp;quot;:{\&amp;quot;filtered\&amp;quot;:{\&amp;quot;query\&amp;quot;:{\&amp;quot;text\&amp;quot;:{\&amp;quot;&amp;quot;)
                            .append(field).append(&amp;quot;\&amp;quot;:\&amp;quot;&amp;quot;).append(term)
                            .append(&amp;quot;\&amp;quot;}},\&amp;quot;filter\&amp;quot;:{\&amp;quot;term\&amp;quot;:{&amp;quot;).append(filter).append(&amp;quot;}}}}}&amp;quot;);
                } else {
                    sb.append(&amp;quot;{\&amp;quot;fields\&amp;quot;:&amp;quot;);
                    append(sb, resultfields);
                    sb.append(&amp;quot;,\&amp;quot;query\&amp;quot;:{\&amp;quot;text\&amp;quot;:{\&amp;quot;&amp;quot;)
                            .append(field).append(&amp;quot;\&amp;quot;:\&amp;quot;&amp;quot;).append(term)
                            .append(&amp;quot;\&amp;quot;}}}&amp;quot;);
                }
                action.search(sb.toString());
            }
        };
    }

    private void append(StringBuilder sb, List&amp;lt;String&amp;gt; list) {
        boolean first = true;
        sb.append(&amp;quot;[&amp;quot;);
        for (String s : list) {
            if (!first) {
                sb.append(&amp;#39;,&amp;#39;);
            }
            sb.append(&amp;quot;\&amp;quot;&amp;quot;).append(s).append(&amp;quot;\&amp;quot;&amp;quot;);
            first = false;
        }
        sb.append(&amp;quot;]&amp;quot;);
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h1&gt;The jQuery front page&lt;/h1&gt;&lt;p&gt;With the autocompleter written in Java JAX-RS, it is more easy to understand the jQuery part of the autocompletion architecture. This HTML page is a very basic jQuery example. It is enriched with HTML autocompletion because we find it useful to add icons to the library title catalog information to signal the media type to the user.&lt;/p&gt;&lt;p&gt;A somewhat complex part is the construction of the title label for linewise display in the drop down area. You can safely ignore that part. It is enough to know the structure of the metadata of a library catalog is hierarchically organized, where title information forms a sequence of objects.&lt;/p&gt;&lt;p&gt;Note how the image icon is excluded from the autocompletion value, since we should pass only textual information to the finl query construction, when the user selects an entry from the autocompletion offer.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
    &amp;lt;head&amp;gt;
        &amp;lt;meta charset=&amp;quot;UTF-8&amp;quot;/&amp;gt;
        &amp;lt;link type=&amp;quot;text/css&amp;quot; href=&amp;quot;css/smoothness/jquery-ui-1.8.22.custom.css&amp;quot; rel=&amp;quot;Stylesheet&amp;quot; /&amp;gt;
        &amp;lt;script src=&amp;quot;js/jquery-1.8.0.min.js&amp;quot; type=&amp;quot;text/javascript&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
        &amp;lt;script src=&amp;quot;js/jquery-ui-1.8.22.custom.min.js&amp;quot; type=&amp;quot;text/javascript&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
        &amp;lt;script src=&amp;quot;js/jquery.ui.autocomplete.html.js&amp;quot; type=&amp;quot;text/javascript&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;/head&amp;gt;

    &amp;lt;body&amp;gt;

        &amp;lt;div&amp;gt;
            &amp;lt;h1&amp;gt;Autocompletion Demo&amp;lt;/h1&amp;gt;
            &amp;lt;div&amp;gt;
                &amp;lt;label for=&amp;quot;search&amp;quot;&amp;gt;Search &amp;lt;/label&amp;gt;
                &amp;lt;input id=&amp;quot;search&amp;quot;/&amp;gt;
            &amp;lt;/div&amp;gt;
        &amp;lt;/div&amp;gt;

        &amp;lt;script&amp;gt;
            $(function() {
                $(&amp;quot;#search&amp;quot;).autocomplete({
                    html: true,
                    source: function(request, response) {
                        $.ajax({
                            url: &amp;quot;http://xbib.org/services/autocomplete&amp;quot;,
                            type: &amp;quot;post&amp;quot;,
                            data: {
                                field: &amp;quot;xbib:titleAutocomplete&amp;quot;,
                                term: request.term,
                                resultfields: [ &amp;quot;dc:title&amp;quot;,&amp;quot;dc:contributor&amp;quot;,&amp;quot;dc:format&amp;quot; ],
                                from: 0,
                                size: 10
                            },
                            dataType: &amp;quot;json&amp;quot;,
                            success: function(data) {
                                response($.map(data.hits.hits, function(item) {
                                    var v = item.fields[&amp;#39;dc:title&amp;#39;];
                                    var w;
                                    if( Object.prototype.toString.call(v) === &amp;#39;[object Array]&amp;#39; ) {
                                        v.map(function(element, index, array) {
                                            for (var i in element) {
                                                if (element.hasOwnProperty(i)) {
                                                    w = w ? w + &amp;quot; / &amp;quot; + element[i] :element[i];
                                                }
                                            }
                                        })                                     
                                    }
                                    var label = 
                                            (w ? w : item.fields[&amp;#39;dc:title&amp;#39;][&amp;#39;xbib:title&amp;#39;]) + 
                                            (item.fields[&amp;#39;dc:title&amp;#39;][&amp;#39;xbib:titleSub&amp;#39;] ? &amp;quot; / &amp;quot; + item.fields[&amp;#39;dc:title&amp;#39;][&amp;#39;xbib:titleSub&amp;#39;] : &amp;quot;&amp;quot;) +
                                            (item.fields[&amp;#39;dc:contributor&amp;#39;] &amp;amp;&amp;amp; item.fields[&amp;#39;dc:contributor&amp;#39;][&amp;#39;bib:namePersonal&amp;#39;] ? &amp;quot; / &amp;quot; + item.fields[&amp;#39;dc:contributor&amp;#39;][&amp;#39;bib:namePersonal&amp;#39;] : &amp;quot;&amp;quot;) +
                                            (item.fields[&amp;#39;dc:contributor&amp;#39;] &amp;amp;&amp;amp; item.fields[&amp;#39;dc:contributor&amp;#39;][&amp;#39;bib:nameCorporate&amp;#39;] ? &amp;quot; / &amp;quot; + item.fields[&amp;#39;dc:contributor&amp;#39;][&amp;#39;bib:nameCorporate&amp;#39;] : &amp;quot;&amp;quot;) +
                                            (item.fields[&amp;#39;dc:contributor&amp;#39;] &amp;amp;&amp;amp; item.fields[&amp;#39;dc:contributor&amp;#39;][&amp;#39;bib:nameConference&amp;#39;] ? &amp;quot; / &amp;quot; + item.fields[&amp;#39;dc:contributor&amp;#39;][&amp;#39;bib:nameConference&amp;#39;] : &amp;quot;&amp;quot;);
                                    return {
                                        label: &amp;#39;&amp;amp;nbsp;&amp;lt;img alt=&amp;quot;&amp;quot; src=&amp;quot;images/format/&amp;#39; + item.fields[&amp;#39;dc:format&amp;#39;][&amp;#39;dcterms:medium&amp;#39;] + &amp;#39;.png&amp;quot;&amp;gt;&amp;lt;/img&amp;gt; &amp;#39; + label,
                                        value: label
                                    }
                                }));
                            }
                        });
                    },
                    minLength: 2
                })
            });
        &amp;lt;/script&amp;gt;

    &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;Examples&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/images/autocomplete-example-1.png&quot; alt=&quot;Typing&quot;&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/images/autocomplete-example-2.png&quot; alt=&quot;Typing&quot;&quot;/&gt;&lt;/p&gt;&lt;h2&gt;References&lt;/h2&gt;&lt;p&gt;[1] Swiffin, A. L., Arnott, J. L., Pickering, J. A., &amp;amp; Newell, A. F. (1987). Adaptive and predictive techniques in a communication prosthesis. Augmentative and Alternative Communication, 3, 181–191&lt;/p&gt;
	  </description>
    </item>
    
    <item>
      <title>Memory-mapped files with Lucene: some more aspects</title>
      <link>http://jprante.github.io/lessons/2012/07/26/Mmap-with-Lucene.html</link>
      <pubDate>Do, 26 Jul 2012 00:00:00 +0200</pubDate>
      <guid isPermaLink="false">lessons/2012/07/26/Mmap-with-Lucene.html</guid>
      <description>
      &lt;p&gt;&lt;img src=&quot;/images/flickr-2142582850-original.jpeg&quot; alt=&quot;Server Mainboard&quot;&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;a href=&quot;http://creativecommons.org/licenses/by/2.0/&quot;&gt;Licensed under Creative Commons&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;In a great article about using memory-mapped files in Lucene, Uwe Schindler discussed &lt;a href=&quot;http://blog.thetaphi.de/2012/07/use-lucenes-mmapdirectory-on-64bit.html&quot;&gt;why switching Lucene directories to MMapDirectory&lt;/a&gt; is a good thing today.&lt;/p&gt;&lt;p&gt;Some questions have arisen in the article and some thoughts came up in my mind, so I decided to write a little more aspects here about virtual memory.&lt;/p&gt;&lt;p&gt;One question that came up to me was: why are memory-mapped files a better choice over paging - since both of them are managed by the OS, and both try to optimize memory resource usage between memory and external disks?&lt;/p&gt;&lt;p&gt;Will &lt;code&gt;RAMDirectory&lt;/code&gt; be a good choice for keeping Lucene index in memory?&lt;/p&gt;&lt;p&gt;Or: is every 64bit system suitable for memory-mapped files?&lt;/p&gt;&lt;p&gt;Another one: do Linux/Solaris offer adequate solutions to the challenges of switching to &lt;code&gt;mmap&lt;/code&gt;&apos;ed Lucene directories?&lt;/p&gt;&lt;p&gt;Before stepping into trying to find answers, let me quickly resume what the paging feature is as we know it from UNIX-like operating systems.&lt;/p&gt;&lt;h2&gt;Paging&lt;/h2&gt;&lt;p&gt;Paging takes place on the operating system (OS) level. The OS has to manage process code, data, and several types of cache, for instance the file system cache, so they can exist side-by-side in the virtual memory address space. Because the OS knows about what is going on in the whole system, it can take care of the right priorities. Memory pages can be moved to RAM or they can be evicted to external storage. Applications in the user space can not force the OS to change paging strategies. Paged data can be adressed only by the address of the page. When memory pages are swapped, they usually can not be shared between processes.&lt;/p&gt;&lt;p&gt;Swapping or (paging) is generally known as a performance killer because it can cause high I/O activity on swap devices or files on disk. But paging is not evil. It is beneficial in many cases. Long lived application and large amounts of rarely demanded memory can be swapped to disk. By doing this, the OS is able to fight back against bad inefficient programming style and bad behaving processes. In such cases, paging works like a magic cache, just delivering the right memory resources at the right time to the right place.&lt;/p&gt;&lt;p&gt;But the page cache is not a usual cache. It is a high-priority kernel task that manages paging. And it also depends on the operating system how paging works. Solaris, Linux, Windows, Mac OS X have all different paging behavior. &lt;/p&gt;&lt;p&gt;Why is disabling paging useful? There are two kinds of applications for limiting paging: high-security (to keep secrets like encryption keys in memory) and real-time applications to improve low latency. Search engines like Lucene belong to the latter.&lt;/p&gt;&lt;p&gt;Paging can be disabled by super user privileges. But when paging is disabled, the memory pressure gets higher and the OS has lower file system cache available because it must compete with all the process code and data more frequently. It depends on your OS if preventing paging is desirable, but one fact is, the kernel will spend more time in overhead routines for managing memory allocation.&lt;/p&gt;&lt;p&gt;On Linux, for example, you could put a lot of RAM into your machine to ensure the amount of RAM is greater than your applications will ever need and you can disable swap. If you want to disable swap, terminate all processes that use swap, and execute the command (as root):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# swapoff -a
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;On Solaris, you should be aware that &lt;code&gt;/tmp&lt;/code&gt; is located in the swap area and writing to &lt;code&gt;/tmp&lt;/code&gt; may conflict with the idea of completely disabling paging. Additional issues may arise when using ZFS. ZFS uses an ARC (adaptive replacement cache) but not the normal page cache. The ARC lives in the kernel space. By default, ARC is allocating all available memory for aggressive caching, where swap space is managed by ZFS devices. The result is extra memory pressure when applications with large heap requirements are present, like Lucene. To remedy the situation, it is often recommended to reduce the ARC size so that ARC plus other memory use fits into total RAM size. Use at least Solaris 10 10/09, and for example, you can limit ARC cache usage with adding a line such &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;set zfs:zfs_arc_max 0x780000000
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;to /etc/system. While &lt;code&gt;mmap()&lt;/code&gt; usually accesses the page cache and not ARC, it is possible that the OS may need to keep two copies of the data around. The latest ZFS implementations try to avoid such issues.&lt;/p&gt;&lt;h2&gt;Memory footprint of a JVM process&lt;/h2&gt;&lt;p&gt;For understanding better where the JVM puts memory-mapped files, let&apos;s examine the memory organization of a JVM process. Mainly it consists of:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the heap. The maximum size is given by the flag &lt;code&gt;-Xmx&lt;/code&gt;. This is where Java objects are allocated.&lt;/li&gt;
  &lt;li&gt;the permanent generation heap (&quot;perm gen&quot;). The maximum size is given by the flag &lt;code&gt;-XX:MaxPermSize&lt;/code&gt;. This is where the class metadata objects, interned strings (String.intern), and symbol data are allocated.&lt;/li&gt;
  &lt;li&gt;the code cache. The JIT compiled native code is allocated here.&lt;/li&gt;
  &lt;li&gt;memory mapped .jar and .so files. The JDK&apos;s standard class library and application&apos;s jar files are often memory mapped. Various shared library and application shared library files are also memory mapped.&lt;/li&gt;
  &lt;li&gt;thread stacks. The maximum size is given by flag &lt;code&gt;-Xss&lt;/code&gt; or &lt;code&gt;-X:ThreadStackSize&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;the C/malloc heap. Both the JVM itself and any native code typically uses &lt;code&gt;malloc&lt;/code&gt; to allocate memory from this heap. NIO direct buffers are allocated via malloc.&lt;/li&gt;
  &lt;li&gt;any other &lt;code&gt;mmap&lt;/code&gt; calls. Native code can allocate pages in the address space using &lt;code&gt;mmap&lt;/code&gt; (or Java code via JNA)&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Most of these allocations are allocated in terms of virtual memory early, but committed only on demand. Your application&apos;s physical memory use may look low at start time, but may get higher later on.&lt;/p&gt;&lt;p&gt;We learned that memory-mapped files in Lucene allocate pages outside of the Java code heap spaces, instead it allocates them directly in the system&apos;s virtual address space.&lt;/p&gt;&lt;h2&gt;Memory-mapping&lt;/h2&gt;&lt;p&gt;A memory-mapped file is virtual memory which has been assigned a direct byte-for-byte correlation with some portion of a resource. This resource is typically a file that is physically present on-disk, but can also be a device, shared memory object, or other resource that the OS can reference through a file descriptor.&lt;/p&gt;&lt;p&gt;Memory-mapped files can be shared between processes (this might be complex though). When Java directly allocates buffers or maps files to memory, they are allocated outside the Java heap.&lt;/p&gt;&lt;h2&gt;Why &lt;code&gt;mmap&lt;/code&gt; is getting more important today&lt;/h2&gt;&lt;p&gt;One important argument for &lt;code&gt;mmap&lt;/code&gt; is the I/O performance. Much progress has been made in input/output file processing. In recent years, new server machines in the PC class (Intel CPU architecture) were equipped with more powerful memory management but also with new I/O devices. &lt;/p&gt;&lt;p&gt;The challenge was to cope with huge amounts of RAM (because RAM chips were getting cheaper and denser) and with overhauled virtual machine designs where the total address space is virtualized to several logical units that look like a complete hardware layer to the OS. As a result, the memory resources in hardware today can be efficiently managed better than ever. &lt;/p&gt;&lt;p&gt;Another trend in overall system performance is that compiler tool chains and virtual machine architectures (like the JVM) got tremendously better optimization techniques for local code and local data, avoiding much of cache misses and page faults that lead to paging. For applications with large heaps like Lucene, avoiding unnecessary paging is important. It can slow down the overall performance of a machine significantly.&lt;/p&gt;&lt;h2&gt;64bit hardware - it evolved over time&lt;/h2&gt;&lt;p&gt;The largest memory address you can point to with a 32bit pointer is 4GB. But is it really enough to know that 64bit operating systems are using 64bit address pointers? The answer is, it depends. Some engineering decisions for the hardware of our PC &quot;industry standard&quot; server a few years ago regarding memory subsystems were suboptimal, mostly because it was too expensive for the vendors to build such machines. UNIX servers kept for some time their advantage in virtual memory management over PC server. But today, the situation has much improved in favor of the PC servers.&lt;/p&gt;&lt;p&gt;Yesterday&apos;s 64bit hardware was&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;limited by RAM slots, maximum capacity of 4 or 8 GB&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;not able to push large gigabytes of data between CPU and memory subsystems at high speed&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;MMU address space: 43bit (PowerMac, SUN UltraSPARC III) = 8TB, 48bit (AMD64) = 256 TB&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;limited by address bus width: most manufacturers did not exploit the address space beyond 4 GB because it was too expensive. For Intel PCs in the pre-AMD64 era, PAE (page address extension) allowed for addressing up to 64 GB&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;and it was limited when reading or writing to external storage devices (by SCSI speed and drives)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;whereas today&apos;s 64bit hardware&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;have lots of memory banks in machines, with more than terabyte capacity&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;have blazingly fast buses for transports between CPU and main memory subsystems&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;have MMU integrated into the CPU (HyperTransport, QuickPath Interconnect) with serial data paths that can connect to anything on the main board&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;and last not least they have very fast read and write speeds on external storage devices with a improved bus architecture and buffer hierarchy (SAS, SSD)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;And fortunately, beside the application code, the operating systems were improved.&lt;/p&gt;&lt;p&gt;So when you are able to throw in as much hardware as you can buy into your system and the OS can scale with it, you will not need bothering too much about the consequences when you enable &lt;code&gt;mmap()&lt;/code&gt;&apos;ed files.&lt;/p&gt;&lt;h2&gt;Memory overcommit&lt;/h2&gt;&lt;p&gt;Memory overcommit is a nifty kernel feature of Linux/BSD/AIX where &lt;code&gt;malloc&lt;/code&gt; never fails. It never returns a NULL pointer. In Linux, this is usually enabled by default. Processes are allowed to allocate more virtual memory than the system actually has, on the hope that they won&apos;t end up using it. If processes try to use more memory than is available, the Out-of-Memory killer (OOM killer) comes in and picks some process to exit it immediately in order to recover memory for the operating system.&lt;/p&gt;&lt;p&gt;Solaris has no memory overcommit feature.&lt;/p&gt;&lt;p&gt;This feature is also relevant for Lucene &lt;code&gt;mmap&lt;/code&gt;&apos;ed processes. When they request memory, it is likely they will not fail. But later on, when the process memory is going to be used, the OOM killer might step in and kill the Lucene process.&lt;/p&gt;&lt;h2&gt;Locking Lucene processes to memory with &lt;code&gt;mlockall&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;Memory overcommit does not guarantee a Lucene process can completely reside in memory for low latency. One approach to ensure it is by locking the pages of the Lucene process to memory.&lt;/p&gt;&lt;p&gt;&lt;code&gt;mlockall&lt;/code&gt; is a UNIX system call that causes all of the pages mapped by the address space of a process to be memory resident until unlocked or until the process exits or execs another process. &lt;/p&gt;&lt;p&gt;Basically it works like removing paging for this process from your system. With an &lt;code&gt;mlockall&lt;/code&gt;&apos;ed Lucene, you can be sure that your Lucene process will always reside in RAM. When you start a process, call &lt;code&gt;mlockall&lt;/code&gt;, and it won&apos;t fit into RAM, you will get an error, although memory overcommit is enabled.&lt;/p&gt;&lt;p&gt;&lt;code&gt;mlockall&lt;/code&gt; and &lt;code&gt;swapoff -a&lt;/code&gt; have in common that both prevent paging, but one with sharp precision, and the other one like a hammer to the head.&lt;/p&gt;&lt;p&gt;As Linux kernel hacker Robert Love pointed out: &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&quot;Using swapoff for either of the aforementioned use cases is suboptimal. Even if paging is detrimental to a specific application, it is beneficial to the system as a whole. Thus disabling an entire system&apos;s paging isn&apos;t a best practice. If your application holds secrets in memory that you don&apos;t want paged to disk or if your application requires deterministic timing, consider using mlock() or mlockall() to prevent paging.&quot;&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;So finally, we have found out why we should prefer &lt;code&gt;mlockall&lt;/code&gt; and not disabling the entire system&apos;s paging.&lt;/p&gt;&lt;h2&gt;Configuring &lt;code&gt;mmap&lt;/code&gt; and &lt;code&gt;mlockall&lt;/code&gt; in Elasticsearch&lt;/h2&gt;&lt;p&gt;Elasticsearch is my favorite distributed search and indexing implementation based on Lucene.&lt;/p&gt;&lt;p&gt;Example configuration for enabling &lt;code&gt;mmap&lt;/code&gt;&apos;ed Lucene directories in Elasticsearch indexes in elasticsearch.conf: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;index: 
  store: 
    type: mmapfs 
    fs: 
      mmapfs: 
        enabled: true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Elasticsearch allows an &lt;code&gt;mlockall()&lt;/code&gt; call at node booting time with the parameter &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;bootstrap.mlockall: true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;in elasticsearch.yml. To complete the configuration the JVM memory size should also be configured. This is done by editing &lt;code&gt;ES_HEAP_SIZE&lt;/code&gt; in the start script.&lt;/p&gt;&lt;p&gt;&lt;code&gt;bootstrap.mlockall&lt;/code&gt; might cause the JVM or shell session to exit if allocation of the memory fail with &lt;code&gt;unknown mlockall error 0&lt;/code&gt;. Possible reason is that not enough lock memory resources are available on the machine. This can be checked by &lt;code&gt;ulimit -l&lt;/code&gt;. The value should be set to &lt;code&gt;unlimited&lt;/code&gt;.&lt;/p&gt;&lt;h2&gt;Lucene&apos;s &lt;code&gt;RAMDirectory&lt;/code&gt; and Elasticsearch&apos;s &lt;code&gt;ByteBufferDirectory&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;Lucene has an implementation for storing the index in the JVM heap. Right now, the &lt;code&gt;RAMDirectory&lt;/code&gt; is designed only for test purposes and not for production. The reason is &lt;code&gt;RAMDirectory&lt;/code&gt; objects are allocated in the heap by byte chunks, and the buffer size is relatively small (8k). So if your index is a few gigabytes, you have a high number of heap objects and you will encounter garbage collection issues. &lt;code&gt;RAMDirectory&lt;/code&gt; is not a good choice for speeding up index reads and writes under production workloads.&lt;/p&gt;&lt;p&gt;A DirectBufferByte-based clone of Lucene&apos;s MMapDirectory that can store the index outside the heap is currently under development, as suggested by Elasticsearch founder Shay Banon. Elasticsearch already uses &lt;a href=&quot;https://github.com/elasticsearch/elasticsearch/tree/master/src/main/java/org/apache/lucene/store/bytebuffer&quot;&gt;such an implementation&lt;/a&gt; with index store type of &lt;code&gt;memory&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Just change &lt;code&gt;mmapfs&lt;/code&gt; to &lt;code&gt;memory&lt;/code&gt; in the Elasticsearch configuration and enjoy your RAM-only search index, which is also distributed over as many nodes as you wish. It&apos;s simple as that.&lt;/p&gt;&lt;h2&gt;A final word&lt;/h2&gt;&lt;p&gt;I traced the 64-bit server trends now for more than ten years. The future is not hard to anticipate. Servers will be equipped with larger, faster, and denser RAM chips, and terabytes of main memory will become popular. For Lucene-based applications it means that RAM-only installations will become more important.&lt;/p&gt;&lt;p&gt;Uwe Schindler&apos;s article reminded me of the huge progress of recent server technology. I appreciate all the hard work of engineering such extraordinary systems that most annoying resource limits and performance bottlenecks almost have become part of the past.&lt;/p&gt;&lt;p&gt;If you belong to those who are not blessed with the latest and greatest 64bit PC server hardware available or you are simply tied to maintain older OSs and applications, do not expect too much from &lt;code&gt;mmap&lt;/code&gt;&apos;ed Lucene directories. With older hardware, obsolete operating system versions, or tight memory resources, &lt;code&gt;mmap&lt;/code&gt;&apos;ed Lucene directories and &lt;code&gt;mlockall&lt;/code&gt;ed Lucene processes are not always a neat solution.&lt;/p&gt;&lt;h2&gt;References&lt;/h2&gt;&lt;p&gt;Red Hat Enterprise Linux 3: System Administration Guide, &quot;Removing swap space&quot; - &lt;a href=&quot;http://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/3/html/System_Administration_Guide/s1-swap-removing.html&quot;&gt;http://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/3/html/System_Administration_Guide/s1-swap-removing.html&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Ulrich Gräf, &quot;Performance mit ZFS - mit Datenbank Tipps&quot; (in german) - &lt;a href=&quot;http://www.as-systeme.de/sites/default/files/events/zfs_1004.pdf&quot;&gt;http://www.as-systeme.de/sites/default/files/events/zfs_1004.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Hiroshi Yamauchi, &quot;JVM process memory&quot; - &lt;a href=&quot;http://hiroshiyamauchi.blogspot.de/2009/12/jvm-process-memory.html&quot;&gt;http://hiroshiyamauchi.blogspot.de/2009/12/jvm-process-memory.html&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Linus Torvalds, &quot;How big is a 64 bit address space?&quot; - &lt;a href=&quot;http://www.realworldtech.com/forum/?threadid=30389&amp;curpostid=30406&quot;&gt;http://www.realworldtech.com/forum/?threadid=30389&amp;curpostid=30406&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Norm Murray and Neil Horman, &quot;Understanding Virtual Memory&quot; - &lt;a href=&quot;http://www.redhat.com/magazine/001nov04/features/vm/&quot;&gt;http://www.redhat.com/magazine/001nov04/features/vm/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Randy Bryant and Dave O’Hallaron, &quot;Virtual Memory: Systems&quot; - &lt;a href=&quot;http://www.cs.cmu.edu/afs/cs/academic/class/15213-f10/www/lectures/16-vm-systems.pdf&quot;&gt;http://www.cs.cmu.edu/afs/cs/academic/class/15213-f10/www/lectures/16-vm-systems.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;University of Alberta, &quot;Understanding Memory&quot; - &lt;a href=&quot;http://www.ualberta.ca/CNS/RESEARCH/LinuxClusters/mem.html&quot;&gt;http://www.ualberta.ca/CNS/RESEARCH/LinuxClusters/mem.html&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&quot;The Solaris Memory System - Sizing,Tools and Architecture&quot; - &lt;a href=&quot;http://www.solarisinternals.com/si/tools/memtool/vmsizing.pdf&quot;&gt;http://www.solarisinternals.com/si/tools/memtool/vmsizing.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&quot;Respite from the OOM killer&quot; - &lt;a href=&quot;http://lwn.net/Articles/104179/&quot;&gt;http://lwn.net/Articles/104179/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&quot;What are the main differences between mlockall() and swapoff -a?&quot; - &lt;a href=&quot;http://www.quora.com/What-are-the-main-differences-between-mlockall-and-swapoff-a&quot;&gt;http://www.quora.com/What-are-the-main-differences-between-mlockall-and-swapoff-a&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&quot;Elasticsearch guide: Installation&quot; - &lt;a href=&quot;http://www.elasticsearch.org/guide/reference/setup/installation.html&quot;&gt;http://www.elasticsearch.org/guide/reference/setup/installation.html&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&quot;ByteBuffer Directory - allowing to store the index outside the heap&quot; - &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2292&quot;&gt;https://issues.apache.org/jira/browse/LUCENE-2292&lt;/a&gt;&lt;/p&gt;
	  </description>
    </item>
    
    <item>
      <title>Indexing MEDLINE with Elasticsearch</title>
      <link>http://jprante.github.io/lessons/2012/07/09/Indexing-MEDLINE-with Elasticsearch.html</link>
      <pubDate>Mo, 9 Jul 2012 00:00:00 +0200</pubDate>
      <guid isPermaLink="false">lessons/2012/07/09/Indexing-MEDLINE-with Elasticsearch.html</guid>
      <description>
      &lt;p&gt;&lt;img src=&quot;/images/nlm-computer-room.jpg&quot; alt=&quot;NLM&quot;&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;A computer room in the National Library of Medicine, ca. 1976&lt;/em&gt;&lt;/p&gt;&lt;p&gt;The oldest online database is MEDLINE, the primary bibliographic database of the National Library of Medicine (NLM). MEDLINE (and the offline version MEDLARS before) is very interesting since it is popular known and globally used. It is important since it provides access to health science literature and helps doctors in treatments and medical diagnosis. The database is not a universal bibliographic database for a broad range of academic subjects but focuses on a special subject, health and medical sciences.&lt;/p&gt;&lt;p&gt;The idea is to use Elasticsearch for many bibliographic databases because of the schema-less nature. If each special subject database can be retained in its structure, but also got indexed into Elasticsearch, the only challenge remains is the graphical use interface.&lt;/p&gt;&lt;p&gt;Schema-less structured data&lt;/p&gt;&lt;p&gt;On the Web, the most promising approach to handle schema-less structured data is the Resource Description Framework (RDF). RDF&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;pmid:16691963&amp;gt; &amp;lt;nlm:PMID&amp;gt; &amp;quot;16691963&amp;quot; .
&amp;lt;pmid:16691963&amp;gt; &amp;lt;nlm:DateCreated&amp;gt; _:a12600 .
_:a12600 &amp;lt;nlm:Year&amp;gt; &amp;quot;2006&amp;quot; .
_:a12600 &amp;lt;nlm:Month&amp;gt; &amp;quot;06&amp;quot; .
_:a12600 &amp;lt;nlm:Day&amp;gt; &amp;quot;01&amp;quot; .
&amp;lt;pmid:16691963&amp;gt; &amp;lt;nlm:Year&amp;gt; &amp;quot;2006&amp;quot; .
&amp;lt;pmid:16691963&amp;gt; &amp;lt;nlm:Month&amp;gt; &amp;quot;06&amp;quot; .
&amp;lt;pmid:16691963&amp;gt; &amp;lt;nlm:Day&amp;gt; &amp;quot;01&amp;quot; .
&amp;lt;pmid:16691963&amp;gt; &amp;lt;nlm:DateCompleted&amp;gt; _:a12610 .
_:a12610 &amp;lt;nlm:Year&amp;gt; &amp;quot;2006&amp;quot; .
_:a12610 &amp;lt;nlm:Month&amp;gt; &amp;quot;06&amp;quot; .
_:a12610 &amp;lt;nlm:Day&amp;gt; &amp;quot;01&amp;quot; .
&amp;lt;pmid:16691963&amp;gt; &amp;lt;nlm:DateRevised&amp;gt; _:a12618 .
_:a12618 &amp;lt;nlm:Year&amp;gt; &amp;quot;2008&amp;quot; .
_:a12618 &amp;lt;nlm:Month&amp;gt; &amp;quot;11&amp;quot; .
_:a12618 &amp;lt;nlm:Day&amp;gt; &amp;quot;20&amp;quot; .
&amp;lt;pmid:16691963&amp;gt; &amp;lt;nlm:Article&amp;gt; _:a12623 .
_:a12623 &amp;lt;nlm:Journal&amp;gt; _:a12624 .
_:a12624 &amp;lt;nlm:ISSN&amp;gt; &amp;quot;0065-9533&amp;quot; .
_:a12624 &amp;lt;nlm:JournalIssue&amp;gt; _:a12627 .
_:a12627 &amp;lt;nlm:Volume&amp;gt; &amp;quot;9&amp;quot; .
_:a12627 &amp;lt;nlm:PubDate&amp;gt; _:a12629 .
_:a12629 &amp;lt;nlm:Year&amp;gt; &amp;quot;1902&amp;quot; .
_:a12624 &amp;lt;nlm:Title&amp;gt; &amp;quot;Transactions of the American Ophthalmological Society&amp;quot; .
_:a12624 &amp;lt;nlm:ISOAbbreviation&amp;gt; &amp;quot;Trans Am Ophthalmol Soc&amp;quot; .
_:a12623 &amp;lt;nlm:ArticleTitle&amp;gt; &amp;quot;A case of color-blindness in a railroad employee due to tobacco amblyopia, failing to be recognized by any of the wool tests but immediately detected by lamp.&amp;quot; .
_:a12623 &amp;lt;nlm:Pagination&amp;gt; _:a12634 .
_:a12634 &amp;lt;nlm:MedlinePgn&amp;gt; &amp;quot;540-5&amp;quot; .
_:a12623 &amp;lt;nlm:AuthorList&amp;gt; _:a12636 .
_:a12636 &amp;lt;nlm:Author&amp;gt; _:a12637 .
_:a12637 &amp;lt;nlm:LastName&amp;gt; &amp;quot;Thomson&amp;quot; .
_:a12637 &amp;lt;nlm:ForeName&amp;gt; &amp;quot;A G&amp;quot; .
_:a12637 &amp;lt;nlm:Initials&amp;gt; &amp;quot;AG&amp;quot; .
_:a12623 &amp;lt;nlm:Language&amp;gt; &amp;quot;eng&amp;quot; .
_:a12623 &amp;lt;nlm:PublicationTypeList&amp;gt; _:a12648 .
_:a12648 &amp;lt;nlm:PublicationType&amp;gt; &amp;quot;Journal Article&amp;quot; .
&amp;lt;pmid:16691963&amp;gt; &amp;lt;nlm:MedlineJournalInfo&amp;gt; _:a12657 .
_:a12657 &amp;lt;nlm:Country&amp;gt; &amp;quot;United States&amp;quot; .
_:a12657 &amp;lt;nlm:MedlineTA&amp;gt; &amp;quot;Trans Am Ophthalmol Soc&amp;quot; .
_:a12657 &amp;lt;nlm:NlmUniqueID&amp;gt; &amp;quot;7506106&amp;quot; .
_:a12657 &amp;lt;nlm:ISSNLinking&amp;gt; &amp;quot;0065-9533&amp;quot; .
&amp;lt;pmid:16691963&amp;gt; &amp;lt;nlm:OtherID&amp;gt; &amp;quot;PMC1322329&amp;quot; .

{
   &amp;quot;nlm:PMID&amp;quot; : &amp;quot;16691963&amp;quot;,
   &amp;quot;nlm:DateCompleted&amp;quot; : {
      &amp;quot;nlm:Day&amp;quot; : &amp;quot;01&amp;quot;,
      &amp;quot;nlm:Year&amp;quot; : &amp;quot;2006&amp;quot;,
      &amp;quot;nlm:Month&amp;quot; : &amp;quot;06&amp;quot;
   },
   &amp;quot;xbib:info&amp;quot; : &amp;quot;2012-07-09T08:49:46Z&amp;quot;,
   &amp;quot;nlm:Year&amp;quot; : &amp;quot;2006&amp;quot;,
   &amp;quot;xbib:update&amp;quot; : &amp;quot;2012-07-09T08:49:46Z&amp;quot;,
   &amp;quot;nlm:Month&amp;quot; : &amp;quot;06&amp;quot;,
   &amp;quot;nlm:DateRevised&amp;quot; : {
      &amp;quot;nlm:Day&amp;quot; : &amp;quot;20&amp;quot;,
      &amp;quot;nlm:Year&amp;quot; : &amp;quot;2008&amp;quot;,
      &amp;quot;nlm:Month&amp;quot; : &amp;quot;11&amp;quot;
   },
   &amp;quot;nlm:Day&amp;quot; : &amp;quot;01&amp;quot;,
   &amp;quot;nlm:MedlineJournalInfo&amp;quot; : {
      &amp;quot;nlm:ISSNLinking&amp;quot; : &amp;quot;0065-9533&amp;quot;,
      &amp;quot;nlm:NlmUniqueID&amp;quot; : &amp;quot;7506106&amp;quot;,
      &amp;quot;nlm:Country&amp;quot; : &amp;quot;United States&amp;quot;,
      &amp;quot;nlm:MedlineTA&amp;quot; : &amp;quot;Trans Am Ophthalmol Soc&amp;quot;
   },
   &amp;quot;nlm:Article&amp;quot; : {
      &amp;quot;nlm:ArticleTitle&amp;quot; : &amp;quot;A case of color-blindness in a railroad employee due to tobacco amblyopia, failing to be recognized by any of the wool tests but immediately detected by lamp.&amp;quot;,
      &amp;quot;nlm:Pagination&amp;quot; : {
         &amp;quot;nlm:MedlinePgn&amp;quot; : &amp;quot;540-5&amp;quot;
      },
      &amp;quot;nlm:PublicationTypeList&amp;quot; : {
         &amp;quot;nlm:PublicationType&amp;quot; : &amp;quot;Journal Article&amp;quot;
      },
      &amp;quot;nlm:Language&amp;quot; : &amp;quot;eng&amp;quot;,
      &amp;quot;nlm:AuthorList&amp;quot; : {
         &amp;quot;nlm:Author&amp;quot; : {
            &amp;quot;nlm:LastName&amp;quot; : &amp;quot;Thomson&amp;quot;,
            &amp;quot;nlm:Initials&amp;quot; : &amp;quot;AG&amp;quot;,
            &amp;quot;nlm:ForeName&amp;quot; : &amp;quot;A G&amp;quot;
         }
      },
      &amp;quot;nlm:Journal&amp;quot; : {
         &amp;quot;nlm:JournalIssue&amp;quot; : {
            &amp;quot;nlm:Volume&amp;quot; : &amp;quot;9&amp;quot;,
            &amp;quot;nlm:PubDate&amp;quot; : {
               &amp;quot;nlm:Year&amp;quot; : &amp;quot;1902&amp;quot;
            }
         },
         &amp;quot;nlm:ISOAbbreviation&amp;quot; : &amp;quot;Trans Am Ophthalmol Soc&amp;quot;,
         &amp;quot;nlm:Title&amp;quot; : &amp;quot;Transactions of the American Ophthalmological Society&amp;quot;,
         &amp;quot;nlm:ISSN&amp;quot; : &amp;quot;0065-9533&amp;quot;
      }
   },
   &amp;quot;nlm:OtherID&amp;quot; : &amp;quot;PMC1322329&amp;quot;,
   &amp;quot;nlm:DateCreated&amp;quot; : {
      &amp;quot;nlm:Day&amp;quot; : &amp;quot;01&amp;quot;,
      &amp;quot;nlm:Year&amp;quot; : &amp;quot;2006&amp;quot;,
      &amp;quot;nlm:Month&amp;quot; : &amp;quot;06&amp;quot;
   }
}

http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1322329/?tool=pubmed
http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1322329/pdf/taos00146-0142.pdf
http://ukpmc.ac.uk/articles/PMC1322329/pdf/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;a href=&quot;http://dcpapers.dublincore.org/ojs/pubs/article/download/927/923&quot;&gt;http://dcpapers.dublincore.org/ojs/pubs/article/download/927/923&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://www.hbz-nrw.de/dokumentencenter/produkte/dlk/aktuell/vortraege/FAST-Workshop/FAST-Workshop_wo.pdf/&quot;&gt;http://www.hbz-nrw.de/dokumentencenter/produkte/dlk/aktuell/vortraege/FAST-Workshop/FAST-Workshop_wo.pdf/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Medline RDF converter, &lt;a href=&quot;https://bitbucket.org/ww/medline&quot;&gt;https://bitbucket.org/ww/medline&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Richard Jones, Mark MacGillivray, Peter Murray-Rust, Jim Pitman, Peter Sefton, Ben O’Steen and William Waites: &quot;Open Bibliography for Science, Technology, and Medicine&quot;, 2011&lt;br/&gt;&lt;a href=&quot;http://www.dspace.cam.ac.uk/handle/1810/239926&quot;&gt;http://www.dspace.cam.ac.uk/handle/1810/239926&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://hublog.hubmed.org/archives/001949.html&quot;&gt;http://hublog.hubmed.org/archives/001949.html&lt;/a&gt;&lt;br/&gt;&quot;I&apos;d like to build a non-copyrighted list of journals/serials (anything with an ISSN, basically) - including their ISSNs, full titles and abbreviated titles.&quot;&lt;/p&gt;&lt;p&gt;Libraries are challenged with many search tasks since decades. When producing new catalog entries, it has been observed very soon since the mid of the 20th century that it is useful to save work by automatically obtaining already existing entries from the catalog. But how do library professionals find existing catalog entries efficiently?&lt;/p&gt;&lt;p&gt;There are several inventions of library professionals in this field:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;Metadata. Yes, librarians invented &apos;intellectual&apos; metadata. Metadata allows for excerpting important parts from all the information of a media resource like a book. Librarians invented not only subject headings but even compact title and author entities a few centuries before, revolutionizing the publishing industry.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Controlled metadata. Catalog rules were invented to enforce librarians using always the same verbal forms for entity names in catalogs, the &lt;em&gt;authorities&lt;/em&gt;. The form selected for main entries in the catalog is also called &lt;em&gt;heading&lt;/em&gt;. One of the most successful controlled metadata is history are authority files or authority lists [1], such as the Library of Congress Subject Headings (LCSH). In Germany, the first automated authority file was the Körperschaftsdatei of the Zeitschriftendatenbank, invented in the early 1970s.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;The exchange of controlled metadata. Librarians also invented methods to fusion metadata in large databases for harmonization of authority files. A recent impressive one is the &lt;em&gt;Virtual International Authority File&lt;/em&gt; (VIAF). Many nations worldwide can align their entity vocabulary in authority files with the help of VIAF.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;The integration of controlled metadata into the Semantic Web. By using simple &lt;em&gt;Linked Data&lt;/em&gt; techniques as postulated by Tim Berners-Lee, librarians joined the efforts of the &lt;em&gt;World Wide Web Consortium&lt;/em&gt; (W3C) to establish the web as a huge global online information database of facts and statements including library catalogs.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;In Germany, the Deutsche Nationalbibliothek (DNB) just harmonized defragmented german authority files into a single authory file called &lt;em&gt;Gemeinsame Normdatei&lt;/em&gt; (GND). DNB adjusted the catalog rules and assigned URIs to authority entities. This has been recognized as a remarkable step in authority metadata management in german libraries, which has a long history [2], [3].&lt;/p&gt;&lt;h2&gt;Efficient authority search&lt;/h2&gt;&lt;p&gt;But librarians did not care too much for efficient authority search as they go along with their inventions. Up to now, many german libraries offer only rudimentary OPACs with insufficent search capabilities for authority files, dating back on search technology of the 70s of the last century. Such search interfaces are always affected of impedance mismatches due to traditional data formats for cataloging like &lt;em&gt;Machine-readable Cataloging&lt;/em&gt; (MARC) or &lt;em&gt;Maschinelles Austauschformat für Bibliotheken&lt;/em&gt; (MAB).&lt;/p&gt;&lt;p&gt;Lateley, Deutsche Nationalbibliothek prepared GND in an RDF Turtle dump [4], which fits nicely into W3C semantic web and is perfect for indexing into modern search engines. The RDF turtle dump is licensed into the public domain - &lt;em&gt;Creative Commons Zero&lt;/em&gt; (CC0), so everyone can use the data and enrich the data to the fullest extent without having to ask for permission [5].&lt;/p&gt;&lt;p&gt;Updates are offered by DNB via an &lt;em&gt;Open Archives Initiative&lt;/em&gt; (OAI) interface on a daily basis. This is a &quot;pull&quot; mechanism with limited scalability.&lt;/p&gt;&lt;p&gt;It wasn&apos;t too hard to implement an indexer that indexes RDF turtle data in Elasticsearch. The rough components are&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;a turtle parser&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;a namespace environment for parsing&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;a JSON serializer for RDF graphs&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;a bulk indexer for Elasticsearch&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;an OAI river for Elasticsearch for receiving updates&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;As you have noticed, there is no more need to analyze MARC or MAB data or to build schemas for indexing, mapping data from one format to another, and so on. All the tedious boilerplate coding has gone.&lt;/p&gt;&lt;h2&gt;Indexing example&lt;/h2&gt;&lt;p&gt;Here is an example command line how to index the full data set into Elasticsearch. The tool is called&lt;br/&gt;&lt;code&gt;org.xbib.tools.indexer.ElasticsearchGNDIndexer&lt;/code&gt;, the source GND file is &lt;code&gt;GND.ttl.gz&lt;/code&gt;, and the target is an Elasticsearch cluster reachable by the network interface where &lt;code&gt;hostname&lt;/code&gt; is bound to, with the index &lt;code&gt;gnd&lt;/code&gt; and type &lt;code&gt;gnd&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Let&apos;s first create an index &lt;code&gt;gnd&lt;/code&gt; and switch off automatic date field detection.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -XPUT &amp;#39;localhost:9200/gnd&amp;#39; -d &amp;#39;{ &amp;quot;mappings&amp;quot; : { &amp;quot;gnd&amp;quot; : { &amp;quot;date_detection&amp;quot; : false } } }&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then let&apos;s start indexing.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;java -cp target/xbib-tools-1.0-SNAPSHOT-elasticsearchgndindexer.jar org.xbib.tools.indexer.ElasticsearchGNDIndexer --gndfile file:///Users/joerg/Downloads/GND.ttl.gz --elasticsearch &amp;quot;es://hostname:9300&amp;quot; --index gnd --type gnd

Jun 01, 2012 11:04:08 PM org.xbib.elasticsearch.ElasticsearchConnection findClusterName
Information: cluster name found in URI es://hostname:9300?es.cluster.name=oaicluster
Jun 01, 2012 11:04:08 PM org.xbib.elasticsearch.ElasticsearchSession createClient
Information: starting discovery for clustername oaicluster
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.ElasticsearchSession findNodes
Information: adding hostname address for transport client = inet[Jorg-Prantes-MacBook-Pro.local/192.168.1.113:9300]
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.ElasticsearchSession findNodes
Information: hostname address added
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.ElasticsearchSession findNodes
Information: addresses = [inet[Jorg-Prantes-MacBook-Pro.local/192.168.1.113:9300]]
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.ElasticsearchSession findNodes
Information: connected nodes = [[#transport#-1][inet[Jorg-Prantes-MacBook-Pro.local/192.168.1.113:9300]]]
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.ElasticsearchSession findNodes
Information: new connection to #transport#-1 
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 1 requests currently active)
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 2 requests currently active)
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 3 requests currently active)
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 4 requests currently active)
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 5 requests currently active)
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 6 requests currently active)
Jun 01, 2012 11:04:10 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 7 requests currently active)
Jun 01, 2012 11:04:10 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 8 requests currently active)
Jun 01, 2012 11:04:10 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 9 requests currently active)
Jun 01, 2012 11:04:10 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 10 requests currently active)
Jun 01, 2012 11:04:10 PM org.xbib.elasticsearch.BulkWrite processBulk
[..]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The process took roughly 80 minutes on a MacBook Pro for ~9,5 millions of different RDF subjects. The result is a 7.5 GB index in 5 shards (five distributed Lucene indices).&lt;/p&gt;&lt;p&gt;Update: indexing on three servers (HP DL-165 G7, 2x12core Opteron 6172, 16 GB RAM) took 27 minutes for 9.493.987 docs (97.267.642 triples) = 5.860 docs/sec (60.041 triples/sec), result is a 7.7 GB (total of 15.4 GB) index in 24 shards.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/images/elasticsearch-gnd-sample-index.png&quot; alt=&quot;elasticsearch-gnd-sample-index&quot;&quot;/&gt;&lt;/p&gt;&lt;h2&gt;Search example&lt;/h2&gt;&lt;p&gt;Here is an example of a quick search. The result contains two entries of the GND, one from the former Schlagwortdatei and the Gemeinsame Körperschaftsdatei.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -XGET &amp;#39;localhost:9200/gnd/gnd/_search?q=hbz%20nordrhein&amp;#39;

[...]
             {
             &amp;quot;_source&amp;quot; : {
               &amp;quot;gnd:preferredNameForTheCorporateBody&amp;quot; : &amp;quot;Hochschulbibliothekszentrum des Landes Nordrhein-Westfalen &amp;lt;Köln&amp;gt;&amp;quot;,
               &amp;quot;gnd:geographicAreaCode&amp;quot; : &amp;quot;http://d-nb.info/standards/vocab/gnd/geographic-area-code#XA-DE&amp;quot;,
               &amp;quot;rdf:type&amp;quot; : &amp;quot;http://d-nb.info/standards/elementset/gnd#CorporateBody&amp;quot;,
               &amp;quot;gnd:oldAuthorityNumber&amp;quot; : &amp;quot;(DE-588b)2047974-8&amp;quot;,
               &amp;quot;gnd:gndIdentifier&amp;quot; : &amp;quot;2047974-8&amp;quot;,
               &amp;quot;gnd:placeOfBusiness&amp;quot; : &amp;quot;http://d-nb.info/gnd/4031483-2&amp;quot;,
               &amp;quot;gnd:variantNameForTheCorporateBody&amp;quot; : [
                  &amp;quot;Hochschulbibliothekszentrum NRW &amp;lt;Köln&amp;gt;&amp;quot;,
                  &amp;quot;hbz&amp;quot;,
                  &amp;quot;Hochschulbibliothekszentrum &amp;lt;Köln&amp;gt;&amp;quot;
               ]
            },
            &amp;quot;_score&amp;quot; : 1.4516485,
            &amp;quot;_index&amp;quot; : &amp;quot;gnd&amp;quot;,
            &amp;quot;_id&amp;quot; : &amp;quot;http://d-nb.info/gnd/2047974-8&amp;quot;,
            &amp;quot;_type&amp;quot; : &amp;quot;gnd&amp;quot;
         },
         {
            &amp;quot;_source&amp;quot; : {
               &amp;quot;gnd:preferredNameForTheCorporateBody&amp;quot; : &amp;quot;Hochschulbibliothekszentrum des Landes     Nordrhein-Westfalen&amp;quot;,
               &amp;quot;gnd:gndSubjectCategory&amp;quot; : &amp;quot;http://d-nb.info/vocab/gnd-sc#6.7&amp;quot;,
               &amp;quot;rdf:type&amp;quot; : &amp;quot;http://d-nb.info/standards/elementset/gnd#CorporateBody&amp;quot;,
               &amp;quot;gnd:oldAuthorityNumber&amp;quot; : &amp;quot;(DE-588c)4194078-7&amp;quot;,
               &amp;quot;gnd:gndIdentifier&amp;quot; : &amp;quot;4194078-7&amp;quot;,
               &amp;quot;gnd:geographicAreaCode&amp;quot; : &amp;quot;http://d-nb.info/standards/vocab/gnd/geographic-area-code#XA-DE-NW&amp;quot;,
               &amp;quot;gnd:spatialAreaOfActivity&amp;quot; : &amp;quot;http://d-nb.info/gnd/4042570-8&amp;quot;,
               &amp;quot;gnd:topic&amp;quot; : &amp;quot;http://d-nb.info/gnd/4132773-1&amp;quot;,
               &amp;quot;gnd:broaderTermInstantial&amp;quot; : &amp;quot;http://d-nb.info/gnd/4630294-3&amp;quot;,
               &amp;quot;gnd:placeOfBusiness&amp;quot; : &amp;quot;http://d-nb.info/gnd/4031483-2&amp;quot;,
               &amp;quot;gnd:variantNameForTheCorporateBody&amp;quot; : &amp;quot;HBZ&amp;quot;
            },
            &amp;quot;_score&amp;quot; : 1.4498183,
            &amp;quot;_index&amp;quot; : &amp;quot;gnd&amp;quot;,
            &amp;quot;_id&amp;quot; : &amp;quot;http://d-nb.info/gnd/4194078-7&amp;quot;,
            &amp;quot;_type&amp;quot; : &amp;quot;gnd&amp;quot;
         },
[...]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;As you noticed by this quick search, GND may contain duplicates from the predecessing authority files.&lt;/p&gt;&lt;p&gt;It is not a big challenge to transform Elasticsearch JSON search results back to RDF, to N-triples, Turtle, or RDF/XML format, for exporting documents.&lt;/p&gt;&lt;p&gt;As Elasticsearch is only used as a coarse-grained triple store with documents containing sub-graphs of RDF triples with common subject URIs, it is rather tough to implement a SPARQL interface covering all the documents. So, for a full semantic web approach, you have to use a SPARQLified triple store like 4store. But native triple stores do not offer so many useful features like Elasticsearch and are focused on special tasks.&lt;/p&gt;&lt;h2&gt;A river for OAI&lt;/h2&gt;&lt;p&gt;With rivers, Elasticsearch can be extended to fetch data from external sources. I have implemented an &lt;em&gt;Open Archives Initiative&lt;/em&gt; (OAI) river [6]. Such a river acts like an OAI harvester, running unattendedly.&lt;/p&gt;&lt;p&gt;For example an OAI river for GND updates will look like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -XPUT &amp;#39;localhost:9200/_river/gnd/_meta&amp;#39; -d &amp;#39;{&amp;quot;type&amp;quot;:&amp;quot;oai&amp;quot;, &amp;quot;oai&amp;quot;:{&amp;quot;url&amp;quot;:&amp;quot;http://services.dnb.de/oai/repository&amp;quot;,&amp;quot;set&amp;quot;:&amp;quot;authorities&amp;quot;,&amp;quot;metadataPrefix&amp;quot;:&amp;quot;RDFxml&amp;quot;}, &amp;quot;index&amp;quot;:{&amp;quot;index&amp;quot;:&amp;quot;gnd&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;gnd&amp;quot;}}&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will pull GND data on a regular basis from DNB, for example, each hour.&lt;/p&gt;&lt;p&gt;An OAI river in action will give messages like this in the Elasticsearch logfile.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[2012-06-02 12:21:47,252][INFO ][river.oai                ] [Solarr] [oai][gnd] OAI harvest: URL [http://services.dnb.de/oai/repository] set [authorities] metadataPrefix [RDFxml] from [2012-06-02T11:00:00Z] until [2012-06-02T12:00:00Z] resumptionToken [null]
Jun 02, 2012 12:21:47 PM org.xbib.io.http.netty.HttpOperation prepareExecution
Information: method=[GET] uri=[http://services.dnb.de/oai/repository] parameter=[&amp;quot;metadataPrefix=RDFxml&amp;quot;; &amp;quot;set=authorities&amp;quot;; &amp;quot;verb=ListRecords&amp;quot;]
[2012-06-02 12:21:47,887][INFO ][river.oai                ] [Solarr] [oai][gnd] submitting new bulk index request (13 docs, 1 requests currently active)
[2012-06-02 12:21:47,892][INFO ][river.oai                ] [Solarr] [oai][gnd] waiting for 1 active bulk requests
[2012-06-02 12:21:47,910][INFO ][river.oai                ] [Solarr] [oai][gnd] bulk index success (23 millis, 13 docs, total of 26 docs)
[2012-06-02 12:21:47,913][INFO ][river.oai                ] [Solarr] [oai][gnd] next harvest, waiting 1h, URL [http://services.dnb.de/oai/repository] set [authorities] metadataPrefix [RDFxml]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;By receiving updates an a regular basis, the GND search solution is complete.&lt;/p&gt;&lt;h2&gt;Summary&lt;/h2&gt;&lt;p&gt;With powerful search capabilities, Elasticsearch can be turned into an important back-bone for the future libary catalog search infrastructure for the following reasons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;9,5 million GND entries are indexed in minutes&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;search results return in milliseconds&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;growing search and index requirements are not a big challenge&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;a wealth of additional search features such as facet search (drill-down) is availiable which is known to be useful for authority-controlled library catalogs&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;by keeping the RDF data structure intact, Elasticsearch offers scalable RDF literal search&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;using the schema-free and multi-tenancy property of Elasticsearch, it is a platform for maintaining several catalogs in multiple indexes and for aggregating related metadata&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Aggregating metadata is not limited to authority files. Elasticsearch could also aggregate holdings from many libraries. It is even possible to attach full SRU and OAI capabilities to Elasticsearch, turning Elasticsearch into a complete front-end for traditional library systems. This will be shown in one of the subsequent postings here.&lt;/p&gt;&lt;p&gt;This posting is just scraping the surface, but search engines like Elasticsearch can help pushing libraries to a new level of global data harmonization, for example, union catalogs on steroids, or an inter-library loan index. And the improved results of such a globalization will be visible to all of you when you use public and academic libraries all around the world.&lt;/p&gt;&lt;h2&gt;References&lt;/h2&gt;&lt;p&gt;[1] Allen Kent, Harold Lancour: Encyclopedia of Library and Information Science. New York,&lt;br/&gt;Dekker, 1969, Vol. 2, p. 132−138.&lt;/p&gt;&lt;p&gt;[2] &lt;a href=&quot;http://bibliothekarisch.de/blog/2012/05/02/gnd-loest-pnd-gkd-und-swd-ab/&quot;&gt;http://bibliothekarisch.de/blog/2012/05/02/gnd-loest-pnd-gkd-und-swd-ab/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[3] &lt;a href=&quot;http://repositorium.uni-osnabrueck.de/bitstream/urn:nbn:de:gbv:700-201001304634/1/ELibD27_normdateien.pdf&quot;&gt;http://repositorium.uni-osnabrueck.de/bitstream/urn:nbn:de:gbv:700-201001304634/1/ELibD27_normdateien.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[4] &lt;a href=&quot;http://www.dnb.de/DE/Service/DigitaleDienste/LinkedData/linkeddata_node.html&quot;&gt;http://www.dnb.de/DE/Service/DigitaleDienste/LinkedData/linkeddata_node.html&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[5] &lt;a href=&quot;http://creativecommons.org/publicdomain/zero/1.0/deed.de&quot;&gt;http://creativecommons.org/publicdomain/zero/1.0/deed.de&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[6] &lt;a href=&quot;https://github.com/jprante/elasticsearch-river-oai/&quot;&gt;https://github.com/jprante/elasticsearch-river-oai/&lt;/a&gt;&lt;/p&gt;
	  </description>
    </item>
    
    <item>
      <title>Elasticsearch for bibliographic authorities</title>
      <link>http://jprante.github.io/lessons/2012/06/02/Elasticsearch-for-authorities.html</link>
      <pubDate>Sa, 2 Jun 2012 00:00:00 +0200</pubDate>
      <guid isPermaLink="false">lessons/2012/06/02/Elasticsearch-for-authorities.html</guid>
      <description>
      &lt;p&gt;&lt;img src=&quot;/images/Gottfried_Wilhelm_von_Leibniz.jpg&quot; alt=&quot;Leibnix&quot;&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Leibniz, as a librarian, used subject headings in Bibliotheca boineburgica to establish &apos;loci communes&apos;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Libraries are challenged with many search tasks since decades. When producing new catalog entries, it has been observed very soon since the mid of the 20th century that it is useful to save work by automatically obtaining already existing entries from the catalog. But how do library professionals find existing catalog entries efficiently?&lt;/p&gt;&lt;p&gt;There are several inventions of library professionals in this field:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;Metadata. Yes, librarians invented &apos;intellectual&apos; metadata. Metadata allows for excerpting important parts from all the information of a media resource like a book. Librarians invented not only subject headings but even compact title and author entities a few centuries before, revolutionizing the publishing industry.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Controlled metadata. Catalog rules were invented to enforce librarians using always the same verbal forms for entity names in catalogs, the &lt;em&gt;authorities&lt;/em&gt;. The form selected for main entries in the catalog is also called &lt;em&gt;heading&lt;/em&gt;. One of the most successful controlled metadata is history are authority files or authority lists [1], such as the Library of Congress Subject Headings (LCSH). In Germany, the first automated authority file was the Körperschaftsdatei of the Zeitschriftendatenbank, invented in the early 1970s.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;The exchange of controlled metadata. Librarians also invented methods to fusion metadata in large databases for harmonization of authority files. A recent impressive one is the &lt;em&gt;Virtual International Authority File&lt;/em&gt; (VIAF). Many nations worldwide can align their entity vocabulary in authority files with the help of VIAF.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;The integration of controlled metadata into the Semantic Web. By using simple &lt;em&gt;Linked Data&lt;/em&gt; techniques as postulated by Tim Berners-Lee, librarians joined the efforts of the &lt;em&gt;World Wide Web Consortium&lt;/em&gt; (W3C) to establish the web as a huge global online information database of facts and statements including library catalogs.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;In Germany, the Deutsche Nationalbibliothek (DNB) just harmonized defragmented german authority files into a single authory file called &lt;em&gt;Gemeinsame Normdatei&lt;/em&gt; (GND). DNB adjusted the catalog rules and assigned URIs to authority entities. This has been recognized as a remarkable step in authority metadata management in german libraries, which has a long history [2], [3].&lt;/p&gt;&lt;h2&gt;Efficient authority search&lt;/h2&gt;&lt;p&gt;But librarians did not care too much for efficient authority search as they go along with their inventions. Up to now, many german libraries offer only rudimentary OPACs with insufficent search capabilities for authority files, dating back on search technology of the 70s of the last century. Such search interfaces are always affected of impedance mismatches due to traditional data formats for cataloging like &lt;em&gt;Machine-readable Cataloging&lt;/em&gt; (MARC) or &lt;em&gt;Maschinelles Austauschformat für Bibliotheken&lt;/em&gt; (MAB).&lt;/p&gt;&lt;p&gt;Lateley, Deutsche Nationalbibliothek prepared GND in an RDF Turtle dump [4], which fits nicely into W3C semantic web and is perfect for indexing into modern search engines. The RDF turtle dump is licensed into the public domain - &lt;em&gt;Creative Commons Zero&lt;/em&gt; (CC0), so everyone can use the data and enrich the data to the fullest extent without having to ask for permission [5].&lt;/p&gt;&lt;p&gt;Updates are offered by DNB via an &lt;em&gt;Open Archives Initiative&lt;/em&gt; (OAI) interface on a daily basis. This is a &quot;pull&quot; mechanism with limited scalability.&lt;/p&gt;&lt;p&gt;It wasn&apos;t too hard to implement an indexer that indexes RDF turtle data in Elasticsearch. The rough components are&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;a turtle parser&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;a namespace environment for parsing&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;a JSON serializer for RDF graphs&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;a bulk indexer for Elasticsearch&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;an OAI river for Elasticsearch for receiving updates&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;As you have noticed, there is no more need to analyze MARC or MAB data or to build schemas for indexing, mapping data from one format to another, and so on. All the tedious boilerplate coding has gone.&lt;/p&gt;&lt;h2&gt;Indexing example&lt;/h2&gt;&lt;p&gt;Here is an example command line how to index the full data set into Elasticsearch. The tool is called&lt;br/&gt;&lt;code&gt;org.xbib.tools.indexer.ElasticsearchGNDIndexer&lt;/code&gt;, the source GND file is &lt;code&gt;GND.ttl.gz&lt;/code&gt;, and the target is an Elasticsearch cluster reachable by the network interface where &lt;code&gt;hostname&lt;/code&gt; is bound to, with the index &lt;code&gt;gnd&lt;/code&gt; and type &lt;code&gt;gnd&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Let&apos;s first create an index &lt;code&gt;gnd&lt;/code&gt; and switch off automatic date field detection.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -XPUT &amp;#39;localhost:9200/gnd&amp;#39; -d &amp;#39;{ &amp;quot;mappings&amp;quot; : { &amp;quot;gnd&amp;quot; : { &amp;quot;date_detection&amp;quot; : false } } }&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then let&apos;s start indexing.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;java -cp target/xbib-tools-1.0-SNAPSHOT-elasticsearchgndindexer.jar org.xbib.tools.indexer.ElasticsearchGNDIndexer --gndfile file:///Users/joerg/Downloads/GND.ttl.gz --elasticsearch &amp;quot;es://hostname:9300&amp;quot; --index gnd --type gnd

Jun 01, 2012 11:04:08 PM org.xbib.elasticsearch.ElasticsearchConnection findClusterName
Information: cluster name found in URI es://hostname:9300?es.cluster.name=oaicluster
Jun 01, 2012 11:04:08 PM org.xbib.elasticsearch.ElasticsearchSession createClient
Information: starting discovery for clustername oaicluster
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.ElasticsearchSession findNodes
Information: adding hostname address for transport client = inet[Jorg-Prantes-MacBook-Pro.local/192.168.1.113:9300]
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.ElasticsearchSession findNodes
Information: hostname address added
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.ElasticsearchSession findNodes
Information: addresses = [inet[Jorg-Prantes-MacBook-Pro.local/192.168.1.113:9300]]
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.ElasticsearchSession findNodes
Information: connected nodes = [[#transport#-1][inet[Jorg-Prantes-MacBook-Pro.local/192.168.1.113:9300]]]
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.ElasticsearchSession findNodes
Information: new connection to #transport#-1 
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 1 requests currently active)
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 2 requests currently active)
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 3 requests currently active)
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 4 requests currently active)
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 5 requests currently active)
Jun 01, 2012 11:04:09 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 6 requests currently active)
Jun 01, 2012 11:04:10 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 7 requests currently active)
Jun 01, 2012 11:04:10 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 8 requests currently active)
Jun 01, 2012 11:04:10 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 9 requests currently active)
Jun 01, 2012 11:04:10 PM org.xbib.elasticsearch.BulkWrite processBulk
Information: submitting new bulk index request (100 docs, 10 requests currently active)
Jun 01, 2012 11:04:10 PM org.xbib.elasticsearch.BulkWrite processBulk
[..]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The process took roughly 80 minutes on a MacBook Pro for ~9,5 millions of different RDF subjects. The result is a 7.5 GB index in 5 shards (five distributed Lucene indices).&lt;/p&gt;&lt;p&gt;Update: indexing on three servers (HP DL-165 G7, 2x12core Opteron 6172, 16 GB RAM) took 27 minutes for 9.493.987 docs (97.267.642 triples) = 5.860 docs/sec (60.041 triples/sec), result is a 7.7 GB (total of 15.4 GB) index in 24 shards.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/images/elasticsearch-gnd-sample-index.png&quot; alt=&quot;elasticsearch-gnd-sample-index&quot;&quot;/&gt;&lt;/p&gt;&lt;h2&gt;Search example&lt;/h2&gt;&lt;p&gt;Here is an example of a quick search. The result contains two entries of the GND, one from the former Schlagwortdatei and the Gemeinsame Körperschaftsdatei.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -XGET &amp;#39;localhost:9200/gnd/gnd/_search?q=hbz%20nordrhein&amp;#39;

[...]
             {
             &amp;quot;_source&amp;quot; : {
               &amp;quot;gnd:preferredNameForTheCorporateBody&amp;quot; : &amp;quot;Hochschulbibliothekszentrum des Landes Nordrhein-Westfalen &amp;lt;Köln&amp;gt;&amp;quot;,
               &amp;quot;gnd:geographicAreaCode&amp;quot; : &amp;quot;http://d-nb.info/standards/vocab/gnd/geographic-area-code#XA-DE&amp;quot;,
               &amp;quot;rdf:type&amp;quot; : &amp;quot;http://d-nb.info/standards/elementset/gnd#CorporateBody&amp;quot;,
               &amp;quot;gnd:oldAuthorityNumber&amp;quot; : &amp;quot;(DE-588b)2047974-8&amp;quot;,
               &amp;quot;gnd:gndIdentifier&amp;quot; : &amp;quot;2047974-8&amp;quot;,
               &amp;quot;gnd:placeOfBusiness&amp;quot; : &amp;quot;http://d-nb.info/gnd/4031483-2&amp;quot;,
               &amp;quot;gnd:variantNameForTheCorporateBody&amp;quot; : [
                  &amp;quot;Hochschulbibliothekszentrum NRW &amp;lt;Köln&amp;gt;&amp;quot;,
                  &amp;quot;hbz&amp;quot;,
                  &amp;quot;Hochschulbibliothekszentrum &amp;lt;Köln&amp;gt;&amp;quot;
               ]
            },
            &amp;quot;_score&amp;quot; : 1.4516485,
            &amp;quot;_index&amp;quot; : &amp;quot;gnd&amp;quot;,
            &amp;quot;_id&amp;quot; : &amp;quot;http://d-nb.info/gnd/2047974-8&amp;quot;,
            &amp;quot;_type&amp;quot; : &amp;quot;gnd&amp;quot;
         },
         {
            &amp;quot;_source&amp;quot; : {
               &amp;quot;gnd:preferredNameForTheCorporateBody&amp;quot; : &amp;quot;Hochschulbibliothekszentrum des Landes     Nordrhein-Westfalen&amp;quot;,
               &amp;quot;gnd:gndSubjectCategory&amp;quot; : &amp;quot;http://d-nb.info/vocab/gnd-sc#6.7&amp;quot;,
               &amp;quot;rdf:type&amp;quot; : &amp;quot;http://d-nb.info/standards/elementset/gnd#CorporateBody&amp;quot;,
               &amp;quot;gnd:oldAuthorityNumber&amp;quot; : &amp;quot;(DE-588c)4194078-7&amp;quot;,
               &amp;quot;gnd:gndIdentifier&amp;quot; : &amp;quot;4194078-7&amp;quot;,
               &amp;quot;gnd:geographicAreaCode&amp;quot; : &amp;quot;http://d-nb.info/standards/vocab/gnd/geographic-area-code#XA-DE-NW&amp;quot;,
               &amp;quot;gnd:spatialAreaOfActivity&amp;quot; : &amp;quot;http://d-nb.info/gnd/4042570-8&amp;quot;,
               &amp;quot;gnd:topic&amp;quot; : &amp;quot;http://d-nb.info/gnd/4132773-1&amp;quot;,
               &amp;quot;gnd:broaderTermInstantial&amp;quot; : &amp;quot;http://d-nb.info/gnd/4630294-3&amp;quot;,
               &amp;quot;gnd:placeOfBusiness&amp;quot; : &amp;quot;http://d-nb.info/gnd/4031483-2&amp;quot;,
               &amp;quot;gnd:variantNameForTheCorporateBody&amp;quot; : &amp;quot;HBZ&amp;quot;
            },
            &amp;quot;_score&amp;quot; : 1.4498183,
            &amp;quot;_index&amp;quot; : &amp;quot;gnd&amp;quot;,
            &amp;quot;_id&amp;quot; : &amp;quot;http://d-nb.info/gnd/4194078-7&amp;quot;,
            &amp;quot;_type&amp;quot; : &amp;quot;gnd&amp;quot;
         },
[...]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;As you noticed by this quick search, GND may contain duplicates from the predecessing authority files.&lt;/p&gt;&lt;p&gt;It is not a big challenge to transform Elasticsearch JSON search results back to RDF, to N-triples, Turtle, or RDF/XML format, for exporting documents.&lt;/p&gt;&lt;p&gt;As Elasticsearch is only used as a coarse-grained triple store with documents containing sub-graphs of RDF triples with common subject URIs, it is rather tough to implement a SPARQL interface covering all the documents. So, for a full semantic web approach, you have to use a SPARQLified triple store like 4store. But native triple stores do not offer so many useful features like Elasticsearch and are focused on special tasks.&lt;/p&gt;&lt;h2&gt;A river for OAI&lt;/h2&gt;&lt;p&gt;With rivers, Elasticsearch can be extended to fetch data from external sources. I have implemented an &lt;em&gt;Open Archives Initiative&lt;/em&gt; (OAI) river [6]. Such a river acts like an OAI harvester, running unattendedly.&lt;/p&gt;&lt;p&gt;For example an OAI river for GND updates will look like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -XPUT &amp;#39;localhost:9200/_river/gnd/_meta&amp;#39; -d &amp;#39;{&amp;quot;type&amp;quot;:&amp;quot;oai&amp;quot;, &amp;quot;oai&amp;quot;:{&amp;quot;url&amp;quot;:&amp;quot;http://services.dnb.de/oai/repository&amp;quot;,&amp;quot;set&amp;quot;:&amp;quot;authorities&amp;quot;,&amp;quot;metadataPrefix&amp;quot;:&amp;quot;RDFxml&amp;quot;}, &amp;quot;index&amp;quot;:{&amp;quot;index&amp;quot;:&amp;quot;gnd&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;gnd&amp;quot;}}&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will pull GND data on a regular basis from DNB, for example, each hour.&lt;/p&gt;&lt;p&gt;An OAI river in action will give messages like this in the Elasticsearch logfile.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[2012-06-02 12:21:47,252][INFO ][river.oai                ] [Solarr] [oai][gnd] OAI harvest: URL [http://services.dnb.de/oai/repository] set [authorities] metadataPrefix [RDFxml] from [2012-06-02T11:00:00Z] until [2012-06-02T12:00:00Z] resumptionToken [null]
Jun 02, 2012 12:21:47 PM org.xbib.io.http.netty.HttpOperation prepareExecution
Information: method=[GET] uri=[http://services.dnb.de/oai/repository] parameter=[&amp;quot;metadataPrefix=RDFxml&amp;quot;; &amp;quot;set=authorities&amp;quot;; &amp;quot;verb=ListRecords&amp;quot;]
[2012-06-02 12:21:47,887][INFO ][river.oai                ] [Solarr] [oai][gnd] submitting new bulk index request (13 docs, 1 requests currently active)
[2012-06-02 12:21:47,892][INFO ][river.oai                ] [Solarr] [oai][gnd] waiting for 1 active bulk requests
[2012-06-02 12:21:47,910][INFO ][river.oai                ] [Solarr] [oai][gnd] bulk index success (23 millis, 13 docs, total of 26 docs)
[2012-06-02 12:21:47,913][INFO ][river.oai                ] [Solarr] [oai][gnd] next harvest, waiting 1h, URL [http://services.dnb.de/oai/repository] set [authorities] metadataPrefix [RDFxml]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;By receiving updates an a regular basis, the GND search solution is complete.&lt;/p&gt;&lt;h2&gt;Summary&lt;/h2&gt;&lt;p&gt;With powerful search capabilities, Elasticsearch can be turned into an important back-bone for the future libary catalog search infrastructure for the following reasons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;9,5 million GND entries are indexed in minutes&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;search results return in milliseconds&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;growing search and index requirements are not a big challenge&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;a wealth of additional search features such as facet search (drill-down) is availiable which is known to be useful for authority-controlled library catalogs&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;by keeping the RDF data structure intact, Elasticsearch offers scalable RDF literal search&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;using the schema-free and multi-tenancy property of Elasticsearch, it is a platform for maintaining several catalogs in multiple indexes and for aggregating related metadata&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Aggregating metadata is not limited to authority files. Elasticsearch could also aggregate holdings from many libraries. It is even possible to attach full SRU and OAI capabilities to Elasticsearch, turning Elasticsearch into a complete front-end for traditional library systems. This will be shown in one of the subsequent postings here.&lt;/p&gt;&lt;p&gt;This posting is just scraping the surface, but search engines like Elasticsearch can help pushing libraries to a new level of global data harmonization, for example, union catalogs on steroids, or an inter-library loan index. And the improved results of such a globalization will be visible to all of you when you use public and academic libraries all around the world.&lt;/p&gt;&lt;h2&gt;References&lt;/h2&gt;&lt;p&gt;[1] Allen Kent, Harold Lancour: Encyclopedia of Library and Information Science. New York,&lt;br/&gt;Dekker, 1969, Vol. 2, p. 132−138.&lt;/p&gt;&lt;p&gt;[2] &lt;a href=&quot;http://bibliothekarisch.de/blog/2012/05/02/gnd-loest-pnd-gkd-und-swd-ab/&quot;&gt;http://bibliothekarisch.de/blog/2012/05/02/gnd-loest-pnd-gkd-und-swd-ab/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[3] &lt;a href=&quot;http://repositorium.uni-osnabrueck.de/bitstream/urn:nbn:de:gbv:700-201001304634/1/ELibD27_normdateien.pdf&quot;&gt;http://repositorium.uni-osnabrueck.de/bitstream/urn:nbn:de:gbv:700-201001304634/1/ELibD27_normdateien.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[4] &lt;a href=&quot;http://www.dnb.de/DE/Service/DigitaleDienste/LinkedData/linkeddata_node.html&quot;&gt;http://www.dnb.de/DE/Service/DigitaleDienste/LinkedData/linkeddata_node.html&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[5] &lt;a href=&quot;http://creativecommons.org/publicdomain/zero/1.0/deed.de&quot;&gt;http://creativecommons.org/publicdomain/zero/1.0/deed.de&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[6] &lt;a href=&quot;https://github.com/jprante/elasticsearch-river-oai/&quot;&gt;https://github.com/jprante/elasticsearch-river-oai/&lt;/a&gt;&lt;/p&gt;
	  </description>
    </item>
    
    <item>
      <title>Elasticsearch multilingual index analysis setup for library catalog search</title>
      <link>http://jprante.github.io/lessons/2012/05/16/Multilingual-analysis-for-title-search.html</link>
      <pubDate>Mi, 16 Mai 2012 00:00:00 +0200</pubDate>
      <guid isPermaLink="false">lessons/2012/05/16/Multilingual-analysis-for-title-search.html</guid>
      <description>
      &lt;p&gt;&lt;img src=&quot;/images/Brueghel-tower-of-babel.jpg&quot; alt=&quot;TowerOfBabel&quot;&quot;/&gt;&lt;/p&gt;&lt;p&gt;When we recently explored ISBN search, we got some appetite for books, and now we want more, advanced book search. Let&apos;s turn to a library catalog. How do libraries, book stores, publishers and vendors offer a convenient, friendly search for book titles? ISBN is obviously not enough. Favorites for search are title keywords.&lt;/p&gt;&lt;p&gt;In this lesson, we focus on the most important search category, the &lt;em&gt;known item search&lt;/em&gt;. Known item search is a very old concept [1], it&apos;s as old as information retrieval as an academic discipline. It is evident that most people are in need for successful known item search in a catalog. But how can we implement such a title search with Elasticsearch?&lt;/p&gt;&lt;h2&gt;What the users are longing for&lt;/h2&gt;&lt;p&gt;At first, we should remind the characteristics and current expectations of a known-item search from a user&apos;s point of view.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;very fast response&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;it shall be easy-as-pie, require as few title keywords as possible to obtain relevant results&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;it shall give back all the titles that match, but only titles that are most preferred on top&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;it shall allow precise title search, where title keyword search is separated from other kinds of searches like author name search, for instance if a person name appears in a title&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;it shall not be restricted to a localization environment, for example a certain language&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;We will focus on the latter point in this lesson.&lt;/p&gt;&lt;h2&gt;Nature of title word forms&lt;/h2&gt;&lt;p&gt;The nature of title word forms in a library catalog is one of the most challenging that can be imagined. Not only the language of domestic books, which are in majority, but languages from all over the world may appear in the title field, not mentioning artificial and ancient word forms. Moreover, in an online environment, different users from all over the world needs to be served appropriately to find the books in their own language. Ths challenge is often called multilingual search.&lt;/p&gt;&lt;p&gt;Yale university library, which is undoubtedly one of the most experienced library using original languages in cataloging, has examined this topic &lt;a href=&quot;https://collaborate.library.yale.edu/yufind/public/FinalReportPublic.pdf&quot;&gt;in depth&lt;/a&gt; and concluded&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;The current state of development in Unicode, Lucene/Solr and Solrmarc has advanced to a level that makes providing the high-priority original script support features entirely feasible for a relatively modest cost.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;Now, let&apos;s tell our plan with Elasticsearch:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;because most of the users of a german library catalog are german speaking, we implement &quot;expanded umlaut&quot; search: german umlauts (ä, ö, ü), when absent on keyboards, are often entered as ae, oe, or ue, or simply abbreviated as a, o, u. Note this tri-fold search is a very unique requirement, only existent in german language. In Elasticsearch, we select the snowball stemmer with German2 language setting to cope with this [2]&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;because of the wealth of international languages, we add Unicode-based character folding with the help of the International Components for Unicode (ICU) Elasticsearch plugin&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;we fix certain deficiencies of the Snowball stemmer by additional indexing of the unchanged word form&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Here is the Index Analysis setting we choose:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;index&amp;quot; : {
   &amp;quot;analysis&amp;quot; : {
      &amp;quot;filter&amp;quot; : {
         &amp;quot;germansnow&amp;quot; : {
            &amp;quot;language&amp;quot; : &amp;quot;German2&amp;quot;,
            &amp;quot;type&amp;quot; : &amp;quot;snowball&amp;quot;
         }
      },
      &amp;quot;analyzer&amp;quot; : {
         &amp;quot;german&amp;quot; : {
            &amp;quot;filter&amp;quot; : [
               &amp;quot;germansnow&amp;quot;,
               &amp;quot;icu_folding&amp;quot;
            ],
            &amp;quot;type&amp;quot; : &amp;quot;custom&amp;quot;,
            &amp;quot;tokenizer&amp;quot; : &amp;quot;icu_tokenizer&amp;quot;
         },
         &amp;quot;default&amp;quot; : {
            &amp;quot;sub_analyzers&amp;quot; : [
               &amp;quot;standard&amp;quot;,
               &amp;quot;german&amp;quot;
            ],
            &amp;quot;type&amp;quot; : &amp;quot;combo&amp;quot;
         }
      }
   }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Let&apos;s describe what we do here.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;p&gt;We define a &lt;code&gt;germansnow&lt;/code&gt; analysis filter. The filter is of type &lt;code&gt;snowball&lt;/code&gt; and is configured to use the language &lt;code&gt;German2&lt;/code&gt;, which allows for the infamous umlaut expansion.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;We define two analyzers, one for german snowball with unicode folding filter appended, the other for combining the standard analyzer and the german analyzer.&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;By setting the latter analyzer name to &lt;code&gt;default&lt;/code&gt;, it is enabled by default. &lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;Note that the order of applying analyzers and filters is significant.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;h2&gt;The Köln Test&lt;/h2&gt;&lt;p&gt;Let&apos;s test this index analysis setting. Elasticsearch has a wonderful diagnostic tool, the Analyze API, for testing how words get transformed before they got indexed.&lt;/p&gt;&lt;p&gt;We check the tri-folding of Köln by using the word forms &lt;code&gt;köln&lt;/code&gt;, &lt;code&gt;koeln&lt;/code&gt;, and &lt;code&gt;koln&lt;/code&gt;. What we require is equivalence of the indexed forms. And we can find that each form is indexed with the reduced (stemmed) form &lt;code&gt;koln&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl &amp;#39;localhost:9200/test/_analyze?text=k%c3%b6ln&amp;amp;pretty=1&amp;#39;
{
  &amp;quot;tokens&amp;quot; : [ {
    &amp;quot;token&amp;quot; : &amp;quot;köln&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 4,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;koln&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 4,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  } ]
}


curl &amp;#39;localhost:9200/test/_analyze?text=koeln&amp;amp;pretty=1&amp;#39;
{
  &amp;quot;tokens&amp;quot; : [ {
    &amp;quot;token&amp;quot; : &amp;quot;koeln&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 5,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;koln&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 5,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  } ]
}

curl &amp;#39;localhost:9200/test/_analyze?text=koln&amp;amp;pretty=1&amp;#39;
{
  &amp;quot;tokens&amp;quot; : [ {
    &amp;quot;token&amp;quot; : &amp;quot;koln&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 4,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;koln&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 4,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  } ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note the original word form is always indexed, together with the stemmed form. So, by showing the analyzed forms of the three word forms of Köln, the requirement of indexing and searching german titles in all various forms is fulfilled.&lt;/p&gt;&lt;h2&gt;Fixing overstemming&lt;/h2&gt;&lt;p&gt;Let&apos;s check for something we call the overstemming phenomenon. It occurs when a language stemmer is producing stemmed forms that are too short for a reasonable search. As a consequence, searches will miss some hits. Quoting Snowball German2 stemmer: &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;Of the native German words, about half seem to be improved by the variant stemming, and the other half made worse. In any case the differences are little more than one word per thousand among the native German words.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;&lt;p&gt;The french word &lt;code&gt;baudelairienne&lt;/code&gt; is an example. If being applied to the Snowball German2 stem algorithm, the last character &lt;code&gt;e&lt;/code&gt; is dropped.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl &amp;#39;localhost:9200/test/_analyze?text=baudelairienne&amp;amp;pretty=1&amp;#39;
{
  &amp;quot;tokens&amp;quot; : [ {
    &amp;quot;token&amp;quot; : &amp;quot;baudelairienne&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 14,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;baudelairienn&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 14,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  } ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It does not surprise us because german stemming is not meant for french words. Interestingly, if the word is given in uppercase, the stem algorithm gives another result - the character is no longer dropped. This is just an example for a possible stem algorithm fault.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl &amp;#39;localhost:9200/test/_analyze?text=BAUDELAIRIENNE&amp;amp;pretty&amp;#39;
{
  &amp;quot;tokens&amp;quot; : [ {
    &amp;quot;token&amp;quot; : &amp;quot;baudelairienne&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 14,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;baudelairienne&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 14,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  } ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now, with the help of the magic combo analyzer, we have combined the standard analysis with the overstemming german Snowball analyzer. As a result, both forms appear in the index, and search hits on both are guaranteed.&lt;/p&gt;&lt;h2&gt;Japanese word forms&lt;/h2&gt;&lt;p&gt;Let&apos;s try japanese word forms. Searches for the name of the japanese capital can be observed in three forms, US-ASCII, transliterated, and japanese: Tokyo, Tōkyō, and 東京.&lt;/p&gt;&lt;p&gt;Unicode folding leads to the effect that transliterated forms get reduced to their base US-ASCII word forms. So, Tōkyō becomes Tokyo in the index.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl &amp;#39;localhost:9200/test/_analyze?text=tokyo&amp;amp;pretty&amp;#39;
{
  &amp;quot;tokens&amp;quot; : [ {
    &amp;quot;token&amp;quot; : &amp;quot;tokyo&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 5,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;tokyo&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 5,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  } ]
}

curl &amp;#39;localhost:9200/test/_analyze?text=t%c5%8dky%c5%8d&amp;amp;pretty&amp;#39;
{
  &amp;quot;tokens&amp;quot; : [ {
    &amp;quot;token&amp;quot; : &amp;quot;tōkyō&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 5,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;tokyo&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 5,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  } ]
}

curl &amp;#39;localhost:9200/test/_analyze?text=%e6%9d%b1%e4%ba%ac&amp;amp;pretty&amp;#39;
{
  &amp;quot;tokens&amp;quot; : [ {
    &amp;quot;token&amp;quot; : &amp;quot;東&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 1,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;IDEOGRAPHIC&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;東&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 1,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;IDEOGRAPHIC&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;京&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 1,
    &amp;quot;end_offset&amp;quot; : 2,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;IDEOGRAPHIC&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 2
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;京&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 1,
    &amp;quot;end_offset&amp;quot; : 2,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;IDEOGRAPHIC&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 2
  } ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that japanese language symbols are stored as ideographic[3] tokens, where delimiting space is treated different from the romanized word forms in the Lucene index.&lt;/p&gt;&lt;h2&gt;Hebrew&lt;/h2&gt;&lt;p&gt;Let&apos;s try some hebrew. This is how the hebrew characters of the city name &lt;em&gt;Tel-Aviv&lt;/em&gt; אביב-יפ is indexed. Note the hebrew alphabet is related to the latin alphabet.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl &amp;#39;localhost:9200/test/_analyze?text=%d7%9c%2d%d7%90%d7%91%d7%99%d7%91&amp;amp;pretty=1&amp;#39;
{
  &amp;quot;tokens&amp;quot; : [ {
    &amp;quot;token&amp;quot; : &amp;quot;ל&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 1,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;ל&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 1,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;אביב&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 2,
    &amp;quot;end_offset&amp;quot; : 6,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 2
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;אביב&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 2,
    &amp;quot;end_offset&amp;quot; : 6,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 2
  } ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;Arabic&lt;/h2&gt;&lt;p&gt;Finally, let&apos;s try analysis of the original script and romanized form of the name of the egyptian capital, al-Qāhira ‏القاهرة‎&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; curl &amp;#39;localhost:9200/test/_analyze?text=%e2%80%8f%d8%a7%d9%84%d9%82%d8%a7%d9%87%d8%b1%d8%a9%e2%80%8e&amp;amp;pretty&amp;#39;
{
  &amp;quot;tokens&amp;quot; : [ {
    &amp;quot;token&amp;quot; : &amp;quot;القاهرة‎&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 1,
    &amp;quot;end_offset&amp;quot; : 9,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;القاهره&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 1,
    &amp;quot;end_offset&amp;quot; : 9,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  } ]
}

curl &amp;#39;localhost:9200/test/_analyze?text=al-Q%c4%81hira&amp;amp;pretty&amp;#39;
{
  &amp;quot;tokens&amp;quot; : [ {
    &amp;quot;token&amp;quot; : &amp;quot;al&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 2,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;al&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 2,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;qāhira&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 3,
    &amp;quot;end_offset&amp;quot; : 9,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 2
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;qahira&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 3,
    &amp;quot;end_offset&amp;quot; : 9,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 2
  } ]
}

curl &amp;#39;localhost:9200/test/_analyze?text=al-Qahira&amp;amp;pretty&amp;#39;
{
  &amp;quot;tokens&amp;quot; : [ {
    &amp;quot;token&amp;quot; : &amp;quot;al&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 2,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;al&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 0,
    &amp;quot;end_offset&amp;quot; : 2,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 1
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;qahira&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 3,
    &amp;quot;end_offset&amp;quot; : 9,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 2
  }, {
    &amp;quot;token&amp;quot; : &amp;quot;qahira&amp;quot;,
    &amp;quot;start_offset&amp;quot; : 3,
    &amp;quot;end_offset&amp;quot; : 9,
    &amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
    &amp;quot;position&amp;quot; : 2
  } ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We reached the end of this lesson. If you are curious about the effectiveness of our index setting, you can try more languages. &lt;/p&gt;&lt;h1&gt;Summary&lt;/h1&gt;&lt;p&gt;We explored indexing words for title searches in a library catalog and learned about the plentitude of languages we have to deal with when we want to offer appropriate search experience to users from all over the world.&lt;/p&gt;&lt;p&gt;By setting up Elasticsearch with a customized index analysis setting, we learned&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;p&gt;that we can succesfully manage difficult localized requirements such as german word stemming with additional umlaut expansion&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;that it is possible to filter tokens not only by word stemming but also by unicode folding provided by the Elasticsearch ICU plugin [4]&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;that words can get overstemmed and need fixing by combining analyzers with the help of the Elasticsearch Combo Analysis plugin [5]&lt;/p&gt;&lt;/li&gt;
  &lt;li&gt;&lt;p&gt;that we can check the indexed word forms with the help of the Elasticsearch analysis API.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Elasticsearch turned out to be a very flexible and powerful tool for library catalog searches. &lt;/p&gt;&lt;p&gt;Author name search is also an interesting topic. In one of the next blog entries we will discuss how to use library authority files such as VIAF or Gemeinsame Normdatei (GND) to enhance Elasticsearch library catalog searches for author names. See you then!&lt;/p&gt;&lt;p&gt;[1] Tagliacozzo, R., Kochen, M. (1970/12).&quot;Information-seeking behavior of catalog users.&quot; Information Storage and Retrieval 6(5): 363-381.&lt;/p&gt;&lt;p&gt;[2] &lt;a href=&quot;http://snowball.tartarus.org/algorithms/german2/stemmer.html&quot;&gt;http://snowball.tartarus.org/algorithms/german2/stemmer.html&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[3] &lt;a href=&quot;http://en.wikipedia.org/wiki/CJK_Unified_Ideographs&quot;&gt;http://en.wikipedia.org/wiki/CJK_Unified_Ideographs&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[4] &lt;a href=&quot;https://github.com/elasticsearch/elasticsearch-analysis-icu&quot;&gt;https://github.com/elasticsearch/elasticsearch-analysis-icu&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[5] &lt;a href=&quot;https://github.com/jprante/elasticsearch-analysis-combo&quot;&gt;https://github.com/jprante/elasticsearch-analysis-combo&lt;/a&gt;&lt;/p&gt;
	  </description>
    </item>
    
    <item>
      <title>ISBN search with Elasticsearch</title>
      <link>http://jprante.github.io/lessons/2012/05/11/ISBN-search-with-Elasticsearch.html</link>
      <pubDate>Fr, 11 Mai 2012 00:00:00 +0200</pubDate>
      <guid isPermaLink="false">lessons/2012/05/11/ISBN-search-with-Elasticsearch.html</guid>
      <description>
      &lt;h1&gt;The challenge: ISBN search&lt;/h1&gt;&lt;p&gt;As most of you know, International Standard Book Numbers (ISBN) come in two representations.&lt;br/&gt;First, there is a legacy form, a short form with ten characters in length.&lt;br/&gt;Those ISBNs appeared exclusively before 2007 and are still very common to most of us.&lt;br/&gt;The fact is that ISBNs were expanded to thirteen characters, since the possible numbers&lt;br/&gt;given by the ISBN specification tended to run out of space. As a matter of fact, each old-style ISBN is now convertible to the new form, and all book merchands, the printing industries, and libraries were advised to only use the new ISBN form. The only ones who can not be advised are the public, that is you and me, who like to search for whatever ISBN comes into the mind.&lt;/p&gt;&lt;p&gt;Anyway, there is a challenge in searching for ISBNs just because of that. There are at least two forms of the same ISBN (some of you who discover printable ISBN forms on the book covers would say there are even more) that need to be searched in parallel. Now, if you carefully store each ISBN form in separate fields, you need to reformulate your search engine queries, guiding the search to all of the various ISBN fields you crafted tediously for your unique best-of-breed book search engine as your users come along.&lt;/p&gt;&lt;p&gt;The thing is, reformulating queries just because of different representations of the same thing is not very clever. Such queries can soon become complex and take search time. Boolean search operations are not very welcome in the eyes of performance knights - you know, they come at a price. And, you need to teach your clients about what fields to use when. This all will take time and energy.&lt;/p&gt;&lt;p&gt;But how to avoid boolean query management, like or&apos;ing all of the fields you want the user to look into? One of the obvious ideas is to combine the ISBN forms into a single field. &lt;/p&gt;&lt;p&gt;You might reply, well, good idea, but combining the forms into a single field is not feasible, because this hinders from searching for each form separately. You won&apos;t be able to deduce from a user&apos;s input what kind of ISBN form was entered. And it&apos;s not hard to imagine a requirement where the user alone is up to decide whether to search for a legacy or new ISBN form. Or, you have an EAN barcode device connected to your search engine, that needs exact evidence about the fact whether an EAN (which is for books equivalent to ISBN-13) exists in the inventory or not.&lt;/p&gt;&lt;p&gt;Now you could resign and go back into your dark and gloomy office, hide from the sun, lock in for months and write nasty and slow code scattered with boolean operations for searching ISBNs. Thinking about waiting for your clients coming by with one legitimate change request after another because suddenly they all turned into ISBN/ISSN/ISMN experts won&apos;t set you into a state of much relief. Or, you could start over and try Elasticsearch and set up a smart configuration that is able to exactly do what all the people will want from you.&lt;/p&gt;&lt;h2&gt;Smart Elasticsearch configuration for ISBN objects&lt;/h2&gt;&lt;p&gt;Elasticsearch JSON offers hierarchical organized properties for your fields in the index, so they can be seen as a structure in an object-like manner. Under such a structure, you can sum up child fields that shall belong to a common parent field. &lt;/p&gt;&lt;p&gt;Your first decision is to put all the ISBN forms under a parent called &lt;code&gt;identifier&lt;/code&gt;. Now, you can index many identifiers in a row, just by using a JSON array. &lt;/p&gt;&lt;p&gt;Your second decision is adding children to the &lt;code&gt;identifier&lt;/code&gt; field, named &lt;code&gt;isbn&lt;/code&gt;, &lt;code&gt;ean&lt;/code&gt;, and &lt;code&gt;isbnprintable&lt;/code&gt;. There can be even more children in the future. Later, you can read the Elasticsearch JSON source that is sent back as a response to your search and extract the information about which forms of ISBN are connected to each other. Yes, you will need to do that analysis sooner or later, because of a very simple reason: some books might have more than one ISBN.&lt;/p&gt;&lt;p&gt;Let us restrict ourselves to just one ISBN for simplicity. We assume an ISBN with three different forms. For example, let&apos;s take ISBN 1-932394-28-1. This form is printed on the book you just have in your hands. From this ISBN form, you derive the form 1932394281 (some call it a normalized form, but ISBNs are not really normalized like this) and, because you are a professional, the form 9781932394283, by recalculating the checksum for ISBN-13. The book was published back in 2005, so the publisher did not know about EAN (or ISBN-13), but you remember you were obliged to use this additional form in current applications, too.&lt;/p&gt;&lt;p&gt;So you have designed your ISBN JSON structure for the Elasticsearch document. Let&apos;s see:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
    &amp;quot;identifier&amp;quot; : {
        &amp;quot;isbn&amp;quot; : &amp;quot;1932394281&amp;quot;,
        &amp;quot;ean&amp;quot; : &amp;quot;9781932394283&amp;quot;,
        &amp;quot;isbnprintable&amp;quot; : &amp;quot;1-932394-28-1&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;Redirection with &lt;code&gt;index_name&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;Elasticsearch has two more smart features in the mappings for an index. One is called &lt;em&gt;index_name&lt;/em&gt; and the other one is &lt;em&gt;multi_field&lt;/em&gt;. By combining these two features we are able to accomplish our mission.&lt;/p&gt;&lt;p&gt;With &lt;em&gt;index_name&lt;/em&gt;, &lt;code&gt;ean&lt;/code&gt; values can be redirected to the same index as &lt;code&gt;isbn&lt;/code&gt;. But, by setting &lt;code&gt;multi_field&lt;/code&gt;, the EAN is still kept in a separate index that we call &lt;code&gt;eanonly&lt;/code&gt;. Finally, we add a mapping for &lt;code&gt;isbnprintable&lt;/code&gt; for the third form of representation.&lt;/p&gt;&lt;p&gt;Just have a look at the ISBN demo mapping:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
   &amp;quot;mappings&amp;quot; : {
      &amp;quot;_default_&amp;quot; : {
         &amp;quot;_source&amp;quot; : {
            &amp;quot;enabled&amp;quot; : true
         },
         &amp;quot;_all&amp;quot; : {
            &amp;quot;analyzer&amp;quot; : &amp;quot;default&amp;quot;,
            &amp;quot;enabled&amp;quot; : true
         },
         &amp;quot;properties&amp;quot; : {
            &amp;quot;identifier&amp;quot; : {
               &amp;quot;dynamic&amp;quot; : false,
               &amp;quot;properties&amp;quot; : {
                  &amp;quot;isbn&amp;quot; : {
                     &amp;quot;type&amp;quot; : &amp;quot;string&amp;quot;,
                     &amp;quot;include_in_all&amp;quot; : true
                  },
                  &amp;quot;ean&amp;quot; : {
                     &amp;quot;type&amp;quot; : &amp;quot;multi_field&amp;quot;,
                     &amp;quot;fields&amp;quot; : {
                         &amp;quot;ean&amp;quot; : {
                             &amp;quot;index_name&amp;quot; : &amp;quot;isbn&amp;quot;,
                             &amp;quot;type&amp;quot; : &amp;quot;string&amp;quot;
                         },
                         &amp;quot;eanonly&amp;quot; : {
                             &amp;quot;type&amp;quot; : &amp;quot;string&amp;quot;
                         }
                     }
                  },
                  &amp;quot;isbnprintable&amp;quot; : {
                     &amp;quot;index_name&amp;quot; : &amp;quot;isbn&amp;quot;,
                     &amp;quot;index&amp;quot; : &amp;quot;not_analyzed&amp;quot;,
                     &amp;quot;type&amp;quot; : &amp;quot;string&amp;quot;
                  }
               }
            }
         }
      }
   }
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;Proving the ISBN search&lt;/h2&gt;&lt;p&gt;Our first test case are users who submit queries and do not like to search in a special ISBN field. They just use a single search field, pasting ISBNs in there, and send it off.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
    &amp;quot;query&amp;quot; : {
         &amp;quot;text&amp;quot; : {
             &amp;quot;_all&amp;quot; : &amp;quot;1932394281&amp;quot;
         }
    }
}

{
    &amp;quot;query&amp;quot; : {
         &amp;quot;text&amp;quot; : {
             &amp;quot;_all&amp;quot; : &amp;quot;9781932394283&amp;quot;
         }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Because of the nature of the &lt;code&gt;include_in_all&lt;/code&gt; of the &lt;code&gt;isbn&lt;/code&gt; field, both ISBN and EAN are included into the &lt;code&gt;_all&lt;/code&gt; field. We did the first case.&lt;/p&gt;&lt;p&gt;Now, let&apos;s satisfy our next users who prefer entering ISBNs into a special ISBN field, regardless of what form. We send their searches into the field &lt;code&gt;identifier.isbn&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
    &amp;quot;query&amp;quot; : {
         &amp;quot;text&amp;quot; : {
             &amp;quot;identifier.isbn&amp;quot; : &amp;quot;1932394281&amp;quot;
         }
    }
}

{
    &amp;quot;query&amp;quot; : {
         &amp;quot;text&amp;quot; : {
             &amp;quot;identifier.isbn&amp;quot; : &amp;quot;9781932394283&amp;quot;
         }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Because the &lt;code&gt;ean&lt;/code&gt; field is redirected to the &lt;code&gt;isbn&lt;/code&gt; field, this works well.&lt;/p&gt;&lt;p&gt;And finally, let&apos;s ensure that our barcode scan devices get only hits on EANs in our book inventory.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
    &amp;quot;query&amp;quot; : {
         &amp;quot;text&amp;quot; : {
             &amp;quot;identifier.ean.eanonly&amp;quot; : &amp;quot;9781932394283&amp;quot;
         }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will not return a hit, as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
    &amp;quot;query&amp;quot; : {
         &amp;quot;text&amp;quot; : {
             &amp;quot;identifier.ean.eanonly&amp;quot; : &amp;quot;1932394281&amp;quot;
         }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We left an example with &lt;code&gt;isbnprintable&lt;/code&gt; search as an exercise.&lt;/p&gt;&lt;h1&gt;Summary&lt;/h1&gt;&lt;p&gt;With Elasticsearch, challenges like ISBN search, where multiple representations of the same thing need to get indexed nearby, can be tackled in an elegant and powerful way. You learned &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;that you can design an object-like structure, where you can later easily deduce what forms are connected to a common parent&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Then you found out that &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;with a suitable Elasticsearch mapping, you can redirect field values to other indexes so they get appended to each other&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;And finally&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;with the &lt;code&gt;multi_field&lt;/code&gt; property, you realized that you can even keep such values and put them into a separate index to avoid queries returning false hits.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;You can find a shell-script curl-based executable of the above example &lt;a href=&quot;https://gist.github.com/2662060&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Now, happy book searching with Elasticsearch!&lt;/p&gt;
	  </description>
    </item>
    
    <item>
      <title>Search with suggestions in Elasticsearch</title>
      <link>http://jprante.github.io/lessons/2012/04/03/Suggestions.html</link>
      <pubDate>Di, 3 Apr 2012 00:00:00 +0200</pubDate>
      <guid isPermaLink="false">lessons/2012/04/03/Suggestions.html</guid>
      <description>
      &lt;p&gt;Searching with suggestions is a useful feature for every saerch solution.&lt;/p&gt;&lt;p&gt;Imagine a user enters saerch terms &lt;/p&gt;&lt;p&gt;Elasticsearch is a distributed, scalable, RESTful open-source search engine implementation based on Lucene. One of the most attractive features of Elasticsearch is the extensiblity by writing plugins. &lt;/p&gt;&lt;p&gt;Sometimes it is reported as a disadvantage that Elasticsearch is developed only by a single person, Shay Banon. But it should be emphasized that he equipped Elasticsearch with a convenient plugin mechanism for developers to encourage participation. The plugin mechanism allows other developers the extension of Elasticsearch with a lot of interesting features, such as adding analyzers, adding rivers, or adding connection mechanism to external libraries and systems, without the need to step into each and every detail of the Elasticsearch core.&lt;/p&gt;&lt;p&gt;In this writeup, I want to show how easy it is to make use of some of the most interesting features of Elasticsearch. The example explains&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;how to extend Elasticsearch with a RESTful command called &lt;code&gt;_termlist&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;how to distribute and collect basic data structures within the shard architecture&lt;/li&gt;
  &lt;li&gt;how to expose Lucene&apos;s IndexReader &lt;code&gt;terms()&lt;/code&gt; method via the Elasticsearch API&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The first step before we can get to the gory details is to step back and get an overview about the Elasticsearch philosophy.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We need to understand the API that Elasticsearch uses to receive requests and to send responses.&lt;/li&gt;
  &lt;li&gt;We need an action which distributes the job from a node to the shards.&lt;/li&gt;
  &lt;li&gt;And finally, we need the implementation of the real low-level work on the shard level.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The plan of attack:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Write a Plugin class, for attaching the plugin to the Elasticsearch core&lt;/li&gt;
  &lt;li&gt;Write a REST action class for API interaction&lt;/li&gt;
  &lt;li&gt;Write Request and Reponse classes for the node that receives the REST action&lt;/li&gt;
  &lt;li&gt;Write a Transport action that is dedicated to the process that works on shard level&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Source code&lt;/h2&gt;&lt;p&gt;The source code of the example plugin can be found at &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/jprante/elasticsearch-index-termlist&quot;&gt;https://github.com/jprante/elasticsearch-index-termlist&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;Writing a Plugin class&lt;/h2&gt;&lt;p&gt;For the example plugin, we choose the name &lt;code&gt;termlist&lt;/code&gt;. The class extends &lt;code&gt;AbstractPlugin&lt;/code&gt;. Each plugin may come with modules. Elasticsearch is divided into logical parts which are called modules. For our simple plugin, we need to hook a &lt;code&gt;RestModule&lt;/code&gt; and an &lt;code&gt;ActionModule&lt;/code&gt; into the Elasticsearch core. By using injection, the Elasticsearch core invokes the &lt;code&gt;onModule()&lt;/code&gt; method of our plugin and sets up the necessary structures in the background for us. All we have to do is&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;public void onModule(RestModule module) {
    module.addRestAction(RestTermlistAction.class);
}

public void onModule(ActionModule module) {
    module.registerAction(TermlistAction.INSTANCE, TransportTermlistAction.class);        
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So we have declared three Action classes &lt;code&gt;RestTermlistAction&lt;/code&gt;, &lt;code&gt;TermlistAction&lt;/code&gt;, and &lt;code&gt;TransportTermlistAction&lt;/code&gt; that we need to implement by further steps.&lt;/p&gt;&lt;h2&gt;Writing a REST Action class&lt;/h2&gt;&lt;p&gt;In the action class &lt;code&gt;RestTermlistAction&lt;/code&gt;, we assign the REST URI info (path patterns) to executions, and the main request/response method &lt;code&gt;handleRequest&lt;/code&gt; that is evaluating the query parameters and writing the JSON response.&lt;/p&gt;&lt;p&gt;Here, we just need to fetch the values out of the URI info, transfer it to a global request &lt;code&gt;TermlistRequest&lt;/code&gt;, and invoke a client that will produce a &lt;code&gt;TermListResponse&lt;/code&gt;. Elasticsearch offers a &lt;code&gt;RestXContentBuilder&lt;/code&gt; API for easy JSON wrapping of our response data. The term list we want to generate is an array of strings, which is just built by writing the line&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;builder.array(&amp;quot;terms&amp;quot;, response.getTermlist().toArray());
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When failures occured, we use Elasticsearch &lt;code&gt;XContentThrowableRestResponse&lt;/code&gt; to create a failure response. &lt;/p&gt;&lt;h2&gt;Writing a Request and Reponse class&lt;/h2&gt;&lt;p&gt;The &lt;code&gt;termlist&lt;/code&gt; plugin should work on the whole cluster or on Elasticsearch indexes. This is done by formulating the request &lt;code&gt;TermlistRequest&lt;/code&gt; as an extension of a &lt;code&gt;BroadcastOperationRequest&lt;/code&gt;. This kind of request will be distributed automagically to all the shards that participate in storing the index (or indexes) requested. If no indexes are given in the request, the whole cluster is addressed.&lt;/p&gt;&lt;p&gt;We add a custom parameter &lt;code&gt;field&lt;/code&gt; beside the index parameter, so that we can expose the IndexReader of Lucene filtered by a given field name. Confining term lists to the terms of a single field is a good idea, for instance if there are a lot of unique terms in the index.&lt;/p&gt;&lt;p&gt;Elasticsearch requires us make the request and the response streamable. Streams are used by the network transport layer to serialize the request to the nodes. All we need to do is writing a few code lines for the &lt;code&gt;field&lt;/code&gt; parameter in &lt;code&gt;readFrom(StreamInput in)&lt;/code&gt; and &lt;code&gt;writeTo(StreamOutput out)&lt;/code&gt;. This mechanism is responsible for both submitting and receving the parameters, from node to node.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;TermlistResponse&lt;/code&gt; is going the opposite direction after the work has done, from the participating shards to the node that will produce the API response. At instantiation of the class, we know the numbers of total shards, successful shards, and failed shards, and a set of terms, the termlist for the &lt;code&gt;RestXContentBuilder&lt;/code&gt;. The response must be streamable just like the request class.&lt;/p&gt;&lt;h2&gt;Writing the Action class&lt;/h2&gt;&lt;p&gt;The action class &lt;code&gt;TermlistAction&lt;/code&gt; is a glue code that clamps together &lt;code&gt;TermlistRequest&lt;/code&gt;, &lt;code&gt;TermlistResponse&lt;/code&gt;, and a &lt;code&gt;TermlistRequestBuilder&lt;/code&gt; into a logical unit. All we need to do is taking care of the generics of the class and filling out the instantiation helpers &lt;code&gt;newRequestBuilder()&lt;/code&gt; and &lt;code&gt;newResponse()&lt;/code&gt;. From now on, the Elasticsearch internal client knows how to construct the request and response structures for our action. But up to this point, no shard logic is involved.&lt;/p&gt;&lt;h2&gt;Writing the Transport Action class&lt;/h2&gt;&lt;p&gt;The shard level logic is done by &lt;code&gt;TransportItemlistAction&lt;/code&gt;, a class extending &lt;code&gt;TransportBroadcastOperationAction&lt;/code&gt;. This action class holds the real execution code of our &lt;code&gt;_termlist&lt;/code&gt; command. Here, we can inject an &lt;code&gt;IndicesServer&lt;/code&gt;, which is the magic Elasticsearch doorway to the Lucene IndexReader internals. In the &lt;code&gt;shardOperation&lt;/code&gt; method, we can extract an &lt;code&gt;Engine.Searcher&lt;/code&gt; object for our current shard ID, from which we obtain the Lucene &lt;code&gt;IndexReader&lt;/code&gt; terms in a loop. &lt;/p&gt;&lt;p&gt;After collecting the terms into a string set, we send them back in a &lt;code&gt;ShardTermlistResponse&lt;/code&gt;. &lt;code&gt;ShardTermlistRequest&lt;/code&gt; and &lt;code&gt;ShardTermlistResponse&lt;/code&gt; are sibling classes to &lt;code&gt;TermlistRequest&lt;/code&gt; and &lt;code&gt;TermlistResponse&lt;/code&gt; on the shard level, extending &lt;code&gt;BroadcastShardOperationRequest&lt;/code&gt; and &lt;code&gt;BroadcastShardOperationResponse&lt;/code&gt; respectively.&lt;/p&gt;&lt;p&gt;Besides that, we can control threading and some other shard topics on this level. So it is possible to address only a subset of shards, the primary shards for example, to save resource usage. &lt;/p&gt;&lt;h2&gt;Summary&lt;/h2&gt;&lt;p&gt;We have learned how to write a simple Elasticsearch plugin by understanding that such an extension is separated into three actions, one for REST, one for the node level to orchestrate the shards, and one for the shard level to perform the low-level work. Each level is connected by a pair of Request/Response classes. While the Request/Response classes can stream data, the REST action builds the response in memory with a RestXContentBuilder.&lt;/p&gt;&lt;p&gt;By extending the Request classes, we learned how we can also pass additional parameters we need for our operation. By extending the Response classes, we learned how to pass our own result structure back to the REST API surface.&lt;/p&gt;&lt;p&gt;We are now able to&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;write Elasticsearch basic plugin code&lt;/li&gt;
  &lt;li&gt;extend the REST API of Elasticsearch by a custom command&lt;/li&gt;
  &lt;li&gt;expose Lucene IndexReader to the Elasticsearch API&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Of course, there is a lot more that Elasticsearch can offer by using the plugin architecture. In this text, we could only scratch the surface. Have fun exploring Elasticsearch for more exciting opportunities!&lt;/p&gt;
	  </description>
    </item>
    
    <item>
      <title>How to write a simple plugin for Elasticsearch</title>
      <link>http://jprante.github.io/lessons/2012/03/27/Writing-a-simple-plugin-for-Elasticsearch.html</link>
      <pubDate>Di, 27 Mär 2012 00:00:00 +0200</pubDate>
      <guid isPermaLink="false">lessons/2012/03/27/Writing-a-simple-plugin-for-Elasticsearch.html</guid>
      <description>
      &lt;p&gt;Elasticsearch is a distributed, scalable, RESTful open-source search engine implementation based on Lucene. One of the most attractive features of Elasticsearch is the extensiblity by writing plugins. &lt;/p&gt;&lt;p&gt;Sometimes it is reported as a disadvantage that Elasticsearch is developed only by a single person, Shay Banon. But it should be emphasized that he equipped Elasticsearch with a convenient plugin mechanism for developers to encourage participation. The plugin mechanism allows other developers the extension of Elasticsearch with a lot of interesting features, such as adding analyzers, adding rivers, or adding connection mechanism to external libraries and systems, without the need to step into each and every detail of the Elasticsearch core.&lt;/p&gt;&lt;p&gt;In this writeup, I want to show how easy it is to make use of some of the most interesting features of Elasticsearch. The example explains&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;how to extend Elasticsearch with a RESTful command called &lt;code&gt;_termlist&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;how to distribute and collect basic data structures within the shard architecture&lt;/li&gt;
  &lt;li&gt;how to expose Lucene&apos;s IndexReader &lt;code&gt;terms()&lt;/code&gt; method via the Elasticsearch API&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The first step before we can get to the gory details is to step back and get an overview about the Elasticsearch philosophy.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We need to understand the API that Elasticsearch uses to receive requests and to send responses.&lt;/li&gt;
  &lt;li&gt;We need an action which distributes the job from a node to the shards.&lt;/li&gt;
  &lt;li&gt;And finally, we need the implementation of the real low-level work on the shard level.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The plan of attack:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Write a Plugin class, for attaching the plugin to the Elasticsearch core&lt;/li&gt;
  &lt;li&gt;Write a REST action class for API interaction&lt;/li&gt;
  &lt;li&gt;Write Request and Reponse classes for the node that receives the REST action&lt;/li&gt;
  &lt;li&gt;Write a Transport action that is dedicated to the process that works on shard level&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Source code&lt;/h2&gt;&lt;p&gt;The source code of the example plugin can be found at &lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/jprante/elasticsearch-index-termlist&quot;&gt;https://github.com/jprante/elasticsearch-index-termlist&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;Writing a Plugin class&lt;/h2&gt;&lt;p&gt;For the example plugin, we choose the name &lt;code&gt;termlist&lt;/code&gt;. The class extends &lt;code&gt;AbstractPlugin&lt;/code&gt;. Each plugin may come with modules. Elasticsearch is divided into logical parts which are called modules. For our simple plugin, we need to hook a &lt;code&gt;RestModule&lt;/code&gt; and an &lt;code&gt;ActionModule&lt;/code&gt; into the Elasticsearch core. By using injection, the Elasticsearch core invokes the &lt;code&gt;onModule()&lt;/code&gt; method of our plugin and sets up the necessary structures in the background for us. All we have to do is&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;public void onModule(RestModule module) {
    module.addRestAction(RestTermlistAction.class);
}

public void onModule(ActionModule module) {
    module.registerAction(TermlistAction.INSTANCE, TransportTermlistAction.class);        
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So we have declared three Action classes &lt;code&gt;RestTermlistAction&lt;/code&gt;, &lt;code&gt;TermlistAction&lt;/code&gt;, and &lt;code&gt;TransportTermlistAction&lt;/code&gt; that we need to implement by further steps.&lt;/p&gt;&lt;h2&gt;Writing a REST Action class&lt;/h2&gt;&lt;p&gt;In the action class &lt;code&gt;RestTermlistAction&lt;/code&gt;, we assign the REST URI info (path patterns) to executions, and the main request/response method &lt;code&gt;handleRequest&lt;/code&gt; that is evaluating the query parameters and writing the JSON response.&lt;/p&gt;&lt;p&gt;Here, we just need to fetch the values out of the URI info, transfer it to a global request &lt;code&gt;TermlistRequest&lt;/code&gt;, and invoke a client that will produce a &lt;code&gt;TermListResponse&lt;/code&gt;. Elasticsearch offers a &lt;code&gt;RestXContentBuilder&lt;/code&gt; API for easy JSON wrapping of our response data. The term list we want to generate is an array of strings, which is just built by writing the line&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;builder.array(&amp;quot;terms&amp;quot;, response.getTermlist().toArray());
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When failures occured, we use Elasticsearch &lt;code&gt;XContentThrowableRestResponse&lt;/code&gt; to create a failure response. &lt;/p&gt;&lt;h2&gt;Writing a Request and Reponse class&lt;/h2&gt;&lt;p&gt;The &lt;code&gt;termlist&lt;/code&gt; plugin should work on the whole cluster or on Elasticsearch indexes. This is done by formulating the request &lt;code&gt;TermlistRequest&lt;/code&gt; as an extension of a &lt;code&gt;BroadcastOperationRequest&lt;/code&gt;. This kind of request will be distributed automagically to all the shards that participate in storing the index (or indexes) requested. If no indexes are given in the request, the whole cluster is addressed.&lt;/p&gt;&lt;p&gt;We add a custom parameter &lt;code&gt;field&lt;/code&gt; beside the index parameter, so that we can expose the IndexReader of Lucene filtered by a given field name. Confining term lists to the terms of a single field is a good idea, for instance if there are a lot of unique terms in the index.&lt;/p&gt;&lt;p&gt;Elasticsearch requires us make the request and the response streamable. Streams are used by the network transport layer to serialize the request to the nodes. All we need to do is writing a few code lines for the &lt;code&gt;field&lt;/code&gt; parameter in &lt;code&gt;readFrom(StreamInput in)&lt;/code&gt; and &lt;code&gt;writeTo(StreamOutput out)&lt;/code&gt;. This mechanism is responsible for both submitting and receving the parameters, from node to node.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;TermlistResponse&lt;/code&gt; is going the opposite direction after the work has done, from the participating shards to the node that will produce the API response. At instantiation of the class, we know the numbers of total shards, successful shards, and failed shards, and a set of terms, the termlist for the &lt;code&gt;RestXContentBuilder&lt;/code&gt;. The response must be streamable just like the request class.&lt;/p&gt;&lt;h2&gt;Writing the Action class&lt;/h2&gt;&lt;p&gt;The action class &lt;code&gt;TermlistAction&lt;/code&gt; is a glue code that clamps together &lt;code&gt;TermlistRequest&lt;/code&gt;, &lt;code&gt;TermlistResponse&lt;/code&gt;, and a &lt;code&gt;TermlistRequestBuilder&lt;/code&gt; into a logical unit. All we need to do is taking care of the generics of the class and filling out the instantiation helpers &lt;code&gt;newRequestBuilder()&lt;/code&gt; and &lt;code&gt;newResponse()&lt;/code&gt;. From now on, the Elasticsearch internal client knows how to construct the request and response structures for our action. But up to this point, no shard logic is involved.&lt;/p&gt;&lt;h2&gt;Writing the Transport Action class&lt;/h2&gt;&lt;p&gt;The shard level logic is done by &lt;code&gt;TransportItemlistAction&lt;/code&gt;, a class extending &lt;code&gt;TransportBroadcastOperationAction&lt;/code&gt;. This action class holds the real execution code of our &lt;code&gt;_termlist&lt;/code&gt; command. Here, we can inject an &lt;code&gt;IndicesServer&lt;/code&gt;, which is the magic Elasticsearch doorway to the Lucene IndexReader internals. In the &lt;code&gt;shardOperation&lt;/code&gt; method, we can extract an &lt;code&gt;Engine.Searcher&lt;/code&gt; object for our current shard ID, from which we obtain the Lucene &lt;code&gt;IndexReader&lt;/code&gt; terms in a loop. &lt;/p&gt;&lt;p&gt;After collecting the terms into a string set, we send them back in a &lt;code&gt;ShardTermlistResponse&lt;/code&gt;. &lt;code&gt;ShardTermlistRequest&lt;/code&gt; and &lt;code&gt;ShardTermlistResponse&lt;/code&gt; are sibling classes to &lt;code&gt;TermlistRequest&lt;/code&gt; and &lt;code&gt;TermlistResponse&lt;/code&gt; on the shard level, extending &lt;code&gt;BroadcastShardOperationRequest&lt;/code&gt; and &lt;code&gt;BroadcastShardOperationResponse&lt;/code&gt; respectively.&lt;/p&gt;&lt;p&gt;Besides that, we can control threading and some other shard topics on this level. So it is possible to address only a subset of shards, the primary shards for example, to save resource usage. &lt;/p&gt;&lt;h2&gt;Summary&lt;/h2&gt;&lt;p&gt;We have learned how to write a simple Elasticsearch plugin by understanding that such an extension is separated into three actions, one for REST, one for the node level to orchestrate the shards, and one for the shard level to perform the low-level work. Each level is connected by a pair of Request/Response classes. While the Request/Response classes can stream data, the REST action builds the response in memory with a RestXContentBuilder.&lt;/p&gt;&lt;p&gt;By extending the Request classes, we learned how we can also pass additional parameters we need for our operation. By extending the Response classes, we learned how to pass our own result structure back to the REST API surface.&lt;/p&gt;&lt;p&gt;We are now able to&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;write Elasticsearch basic plugin code&lt;/li&gt;
  &lt;li&gt;extend the REST API of Elasticsearch by a custom command&lt;/li&gt;
  &lt;li&gt;expose Lucene IndexReader to the Elasticsearch API&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Of course, there is a lot more that Elasticsearch can offer by using the plugin architecture. In this text, we could only scratch the surface. Have fun exploring Elasticsearch for more exciting opportunities!&lt;/p&gt;
	  </description>
    </item>
    

  </channel> 
</rss>
